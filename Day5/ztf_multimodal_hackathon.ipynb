{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f5bfef",
   "metadata": {},
   "source": [
    "# Multimodal Early‚ÄëClassification of ZTF Transients  \n",
    "### *A mini ‚ÄúAppleCiDEr‚Äù hackathon*\n",
    "\n",
    "\n",
    "### Get the data here! https://drive.google.com/drive/u/0/folders/19EUnPU4AyN8Y0dF_CYsNHHxz94qjL1q9 \n",
    "\n",
    "You should download all the data_train_multi files.\n",
    "\n",
    "\n",
    "\n",
    "Welcome to the **grand‚Äëfinale hackathon** of the 2025 ZTF Summer School!  \n",
    "In the next few hours you will build, train, and evaluate a **multimodal classifier** that decides‚Äîwithin the **first‚ÄØ10 days** of discovery‚Äîwhat kind of optical transient we are watching.\n",
    "\n",
    "Why does *early* classification matter?\n",
    "\n",
    "* *Rapid follow‚Äëup:* scarce resources (Spectrographs, JWST, HST, VLA‚Ä¶) must be allocated **before** a transient fades.  \n",
    "* *Fast transients physics:* the more data we have, the better we can constrain models.  \n",
    "* *Event brokers:* surveys such as ZTF, Rubin, ... stream millions of alerts/night; automated early triage is critical!\n",
    "\n",
    "We will focus our taxonomy into **five broad classes** to keep training fast for the time scale of the hackathon:\n",
    "\n",
    "| Broad class | Typical zoo members |\n",
    "|-------------|--------------------|\n",
    "| **SNI**  | SN‚ÄØIa, Ib, Ic |\n",
    "| **SNII** | SN‚ÄØII, II‚ÄëP, IIn, II‚Äëb |\n",
    "| **CV** | Cataclysmic Variables |\n",
    "| **AGN** | Active Galactic Nuclei |\n",
    "| **TDE** |  Tidal Disruption Events |\n",
    "\n",
    "In spirit we are reproducing a toy version of  \n",
    "> **‚ÄúApplying Multimodal Learning to Classify Transient Detections Early (AppleCiDEr) I: Dataset, Methods, and Infrastructure‚Äù** ‚Äì *AJ Brown et al., 2025*.\n",
    "\n",
    "Good luck and have fun!\n",
    "\n",
    "As a starting point, we are providing an early development version of AppleCider. The cells are supposed to be more of a guide then a fully functional code; There will be places where you will either have to complete available code or write your own. Feel free to transfer the parts relevant to you to your own coding environment.\n",
    "\n",
    "This version of the **multimodal classifier** combines:\n",
    "\n",
    "| Modality | Encoder | Key Idea |\n",
    "|----------|---------|----------|\n",
    "| **Photometry** | Transformer | temporal self‚Äëattention on light‚Äëcurves |\n",
    "| **Spectra** | CNN | harvesting power from 2D CNNs |\n",
    "| **Image¬†+¬†Metadata** | astroMiNN | Mixture‚Äëof‚ÄëExperts fusion tower |\n",
    "| **Fusion** | Gated late‚Äëfusion | sample‚Äëadaptive weighting of branch logits |\n",
    "  \n",
    "Everything is thoroughly annotated, but if you have any questions about any of the bits, please let us know :) \n",
    "\n",
    "> **Your challenge:** improve BALANCED ACCURACY of the multi-modal classifier. Feel free to exercise all of your creativity and substitute any of the blocks in this code, to implement any technique you like. We left some tips that might be worth exploring on the ‚Äúüí°¬†Try¬†This‚Äù cells sprinkled throughout.\n",
    ">\n",
    "> Happy Hacking!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5a89b",
   "metadata": {},
   "source": [
    "## 0¬†¬∑¬†Setup & Dependencies\n",
    "\n",
    "- We suggest making a fresh conda environment for this challenge.  \n",
    "- If you run locally, uncomment the cell below to install some of the important Python packages.\n",
    "\n",
    "Feel free to add any extra libraries you need (like `tqdm`, `rich`, `torchmetrics`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72029ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install numpy pandas scikit-learn seaborn wandb tqdm rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f06909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, argparse, warnings, json, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, f1_score,\n",
    "                             precision_score, recall_score, average_precision_score,\n",
    "                             confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
    "                             top_k_accuracy_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "# Pretty¬†plots\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "plt.rcParams['figure.autolayout'] = True\n",
    "wandb.require(\"service\")\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0061528",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit the **data paths** below if you stashed the CSVs or `.npy` files in a different directory.  \n",
    "All other hyper‚Äëparameters are sane defaults, but you are encouraged to play:\n",
    "\n",
    "* Transformer depth, heads, dropout  \n",
    "* CNN kernel sizes / channels  \n",
    "* Number of experts in astroMiNN  \n",
    "* Learning rate schedule, batch size, $\\gamma$ for focal‚Äëloss\n",
    "\n",
    "> **üí° Try This:** crank `d_model` to 256 and halve `epochs` while using cosine restarts; Sometimes, bigger models converge faster, but can be less stable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b09c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # ‚îÄ‚îÄ data paths (EDIT ME) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    csv_path      = \".../data_train_multi_10.csv\"\n",
    "    root_path     = \".../data_train_multi/day10\"\n",
    "    test_csv_path = \".../data_test_multi_10.csv\"\n",
    "    test_root     = \"/data_test_multi/day10\"\n",
    "\n",
    "    # ‚îÄ‚îÄ experiment tracking ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    project       = \"multimodal_late_fusion_gated\"\n",
    "    wandb_mode    = \"disabled\"         # 'disabled' for offline\n",
    "\n",
    "    # ‚îÄ‚îÄ optimisation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    batch_size    = 128\n",
    "    epochs        = 150\n",
    "    lr            = 1e-6\n",
    "    weight_decay  = 1e-2\n",
    "    patience      = 50\n",
    "    num_workers   = 10\n",
    "    gamma         = 2.0              # focal‚Äëloss Œ≥\n",
    "    seed          = 42\n",
    "\n",
    "    # ‚îÄ‚îÄ photometry Transformer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    d_model       = 128\n",
    "    num_layers    = 8\n",
    "    num_heads     = 4\n",
    "    dropout       = 0.3\n",
    "    max_len       = 500\n",
    "\n",
    "    # ‚îÄ‚îÄ spectra CNN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    grid_size           = 59\n",
    "    conv1_filters       = 32\n",
    "    conv1_kernel        = 7\n",
    "    conv2_filters       = 64\n",
    "    conv2_kernel        = 7\n",
    "    pool_size           = 2\n",
    "    fc_units_spec       = 2048\n",
    "    fc_dropout          = 0.3\n",
    "\n",
    "    # ‚îÄ‚îÄ astroMiNN (image + metadata) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    astro_input_dim           = 9\n",
    "    astro_hidden_dim          = 128\n",
    "    astro_proj_dim            = 256\n",
    "    astro_num_experts         = 10\n",
    "    astro_num_fusion_experts  = 5\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# Determinism helpers\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "random.seed(cfg.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ taxonomy ---------------------------------------------------------------\n",
    "BROAD_CLASSES = [\"SNI\", \"SNII\", \"Cataclysmic\", \"AGN\", \"TDE\"]\n",
    "NUM_CLASSES   = len(BROAD_CLASSES)\n",
    "ORIG2BROAD = {\n",
    "    \"SN Ia\":\"SNI\",\"SN Ib\":\"SNI\",\"SN Ic\":\"SNI\",\n",
    "    \"SN II\":\"SNII\",\"SN IIP\":\"SNII\",\"SN IIn\":\"SNII\",\"SN IIb\":\"SNII\",\n",
    "    \"Cataclysmic\":\"Cataclysmic\",\"AGN\":\"AGN\",\"Tidal Disruption Event\":\"TDE\"\n",
    "}\n",
    "BROAD2ID = {c:i for i,c in enumerate(BROAD_CLASSES)}\n",
    "\n",
    "# ‚îÄ‚îÄ pre‚Äëcomputed metadata normalisation stats ------------------------------\n",
    "METADATA_STATS = {\n",
    "    'sgscore1': {'mean': 0.236, 'std': 0.266},\n",
    "    'sgscore2': {'mean': 0.401, 'std': 0.328},\n",
    "    'distpsnr1': {'mean': 3.151, 'std': 3.757},\n",
    "    'distpsnr2': {'mean': 9.269, 'std': 6.323},\n",
    "    'nmtchps': {'mean': 9.231, 'std': 8.089},\n",
    "    'sharpnr': {'mean': 0.253, 'std': 0.256},\n",
    "    'scorr': {'mean': 22.089, 'std': 16.757},\n",
    "    'ra': {'mean': 0, 'std': 1},\n",
    "    'dec': {'mean': 0, 'std': 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc3503",
   "metadata": {},
   "source": [
    "## 2¬†¬∑¬†Data¬†Exploration\n",
    "\n",
    "\n",
    "Before touching any code, let‚Äôs *look* at the dataset. Your first exercise is to obtain as much information from the dataset as you can by plotting data for individual objects, statistics of relevant features...\n",
    "\n",
    "* **Rows:** individual ZTF objects (one per transient)  \n",
    "* **file:** `.npy` bundle containing the four modalities  \n",
    "* **type:** label map \n",
    "\n",
    "As a first suggestion, let‚Äôs peek at the CSV that indexes our multimodal samples and visualise the class distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(cfg.csv_path)\n",
    "full_df['type'] = full_df['type'].map(ORIG2BROAD)\n",
    "display(full_df.head())\n",
    "\n",
    "print(f\"Total samples: {len(full_df):,}\")\n",
    "class_counts = full_df['type'].value_counts().reindex(BROAD_CLASSES).fillna(0)\n",
    "class_counts.plot(kind='bar', rot=0)\n",
    "plt.title('Class Distribution'); plt.ylabel('# objects'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a8ac3-76b4-4456-a98d-d683f1c03810",
   "metadata": {},
   "source": [
    "> **üí° Try¬†This:** Ask yourself whether this level of class imbalance will significantly affect your training. If yes, what techniques could you employ to fight it? Oversampling the minority classes or Undersampling the majority classes are two common techniques, but there are many others out there!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00396db2",
   "metadata": {},
   "source": [
    "## 3 ¬∑ Dataset¬†&¬†Dataloaders\n",
    "\n",
    "### Anatomy of one sample\n",
    "\n",
    "| Tensor | Shape | Meaning |\n",
    "|--------|-------|---------|\n",
    "| `phot` | `(T,‚ÄØ4)` | ZTF light‚Äëcurve (g/r flux, flux‚Äëerr, mjd) |\n",
    "| `spec_img` | `(1,‚ÄØ59,‚ÄØ59)` | Raw spectrum ‚ñ∏ z‚Äëscore ‚ñ∏ square grid |\n",
    "| `meta_vec` | `(9,)` | Contextual features (sgscore, distpsnr, ‚Ä¶) |\n",
    "| `image` | `(3,‚ÄØH,‚ÄØW)` | Triplet of Image Cutouts, scaled to¬†[0,‚ÄØ1] |\n",
    "| `label` | scalar | 0‚Äì4 for the 5 broad classes |\n",
    "\n",
    "Key implementation tricks:\n",
    "\n",
    "1. Padding¬†+¬†masking for variable‚Äëlength photometry sequences so the Transformer ignores padded tokens.  \n",
    "2. StandardScaler on spectra voxels‚Äîfitted on *training* set only (very important, to avoid leakage!).  \n",
    "3. On‚Äëthe‚Äëfly square‚Äëgrid mapping so you can easily swap‚Äëin different `grid_size` or even 1‚ÄëD CNNs.\n",
    "\n",
    "> **üí°¬†Try¬†This:** If preprocessing the spectra is taking too much performance out of your dataloader, cache the mapped spectra on disk for a decent speed‚Äëup at the cost of some RAM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset(Dataset):\n",
    "    \"\"\"Custom dataset returning five tensors per sample.\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, root: Path,\n",
    "                 phot_mean=None, phot_std=None, spec_scaler=None,\n",
    "                 grid_size=cfg.grid_size, transform=None):\n",
    "        self.df   = df.reset_index(drop=True)\n",
    "        self.root = Path(root)\n",
    "        self.phot_mean = phot_mean\n",
    "        self.phot_std  = phot_std\n",
    "        self.spec_scaler = spec_scaler\n",
    "        self.grid_size   = grid_size\n",
    "        self.transform   = transform\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def _load_sample(self, row):\n",
    "        sample = np.load(self.root / row['file'], allow_pickle=True).item()\n",
    "        phot   = torch.tensor(sample['photometry'], dtype=torch.float32)\n",
    "        spec   = np.nan_to_num(sample['spectra'][0])\n",
    "\n",
    "        # --- metadata vector (z‚Äëscore or min‚Äëmax) ----------\n",
    "        m = sample['metadata']\n",
    "        meta_vec = torch.tensor([\n",
    "            (m['sgscore1']  - METADATA_STATS['sgscore1']['mean']) / METADATA_STATS['sgscore1']['std'],\n",
    "            (m['sgscore2']  - METADATA_STATS['sgscore2']['mean']) / METADATA_STATS['sgscore2']['std'],\n",
    "            (m['distpsnr1'] - METADATA_STATS['distpsnr1']['mean']) / METADATA_STATS['distpsnr1']['std'],\n",
    "            (m['distpsnr2'] - METADATA_STATS['distpsnr2']['mean']) / METADATA_STATS['distpsnr2']['std'],\n",
    "            (m['nmtchps']   - METADATA_STATS['nmtchps']['mean'])  / METADATA_STATS['nmtchps']['std'],\n",
    "            (m['sharpnr']   - METADATA_STATS['sharpnr']['mean'])  / METADATA_STATS['sharpnr']['std'],\n",
    "            (m['scorr']     - METADATA_STATS['scorr']['mean'])    / METADATA_STATS['scorr']['std'],\n",
    "            (m['ra'] / 360.0),\n",
    "            ((m['dec'] + 90.0) / 180.0)\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        # --- PSF stamp -------------------------------------\n",
    "        img = torch.tensor(sample['images'], dtype=torch.float32).permute(2,0,1) / 255.0\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return phot, spec, meta_vec, img\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phot, spec, meta_vec, img = self._load_sample(row)\n",
    "\n",
    "        # spectra ‚Üí square grid\n",
    "        spec_std = self.spec_scaler.transform([spec])[0]\n",
    "        needed   = self.grid_size ** 2\n",
    "        if spec_std.shape[0] < needed:\n",
    "            spec_std = np.pad(spec_std, (0, needed - spec_std.shape[0]))\n",
    "        elif spec_std.shape[0] > needed:\n",
    "            spec_std = spec_std[:needed]\n",
    "        spec_img = torch.tensor(spec_std.reshape(self.grid_size, self.grid_size),\n",
    "                                dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label = BROAD2ID[row['type']]\n",
    "        return phot, spec_img, meta_vec, img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phot_mean_std(ds):\n",
    "    \"\"\"Per‚Äëchannel mean/std over an *un‚Äëpadded* photometry dataset.\"\"\"\n",
    "    sums, sqs, n = torch.zeros(4), torch.zeros(4), 0\n",
    "    for phot, *_ in ds:\n",
    "        sums += phot.sum(0)\n",
    "        sqs  += (phot ** 2).sum(0)\n",
    "        n    += phot.shape[0]\n",
    "    mean = (sums / n)[None, None, :]\n",
    "    std  = torch.sqrt(sqs / n - mean.squeeze() ** 2)[None, None, :]\n",
    "    return mean, std\n",
    "\n",
    "def build_collate(mean, std):\n",
    "    def collate(batch):\n",
    "        phot_seqs, spec_imgs, meta_vecs, img_tensors, labels = zip(*batch)\n",
    "\n",
    "        spec_batch = torch.stack(spec_imgs)\n",
    "        meta_batch = torch.stack(meta_vecs)\n",
    "        img_batch  = torch.stack(img_tensors)\n",
    "\n",
    "        lengths = [p.shape[0] for p in phot_seqs]\n",
    "        padded  = pad_sequence(phot_seqs, batch_first=True)\n",
    "        mask    = torch.stack([\n",
    "            torch.cat([torch.zeros(l), torch.ones(padded.shape[1]-l)])\n",
    "            for l in lengths\n",
    "        ]).bool()\n",
    "        normed  = (padded - mean) / (std + 1e-8)\n",
    "        return normed, mask, spec_batch, meta_batch, img_batch, torch.tensor(labels)\n",
    "    return collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afacbe5",
   "metadata": {},
   "source": [
    "## 4 ¬∑ Modal¬†Encoders\n",
    "\n",
    "### Photometry¬†Transformer ¬†\n",
    "Temporal self‚Äëattention excels at capturing irregular cadence and long‚Äërange dependencies.  \n",
    "We prepend a learnable **[CLS] token** whose output embedding summarises the entire light‚Äëcurve.\n",
    "\n",
    "> **üí°¬†Try¬†This:** Can just the [CLS] token hold all the information about the transient classes or would be better to use all output encondings from the Transformer model? \n",
    "\n",
    "\n",
    "> **üí°¬†Try¬†This:** Can we include a better Positional Encoding, that doesn't rely on the index of the photometric point, but actually encodes the timing between observations? Learnable, time-aware positional encoding was shown to do better in time-series. \n",
    "\n",
    "*Input*: padded sequence `x = (time, g_flux, r_flux, i_flux) ‚àà ‚Ñù^{B√óT√ó4}`.   \n",
    "*PosEnc*: $ \\text{PE}(t,2i)=\\sin(t/10000^{2i/d}) $, $ \\text{PE}(t,2i+1)=\\cos(t/10000^{2i/d}) $\n",
    "\n",
    "### Spectra¬†CNN ¬†\n",
    "We reshape each 1‚ÄëD spectrum into a 2‚ÄëD postage‚Äëstamp and apply two convolutional blocks. Cool reference about Spectrum Classification: *DASH* (Muthukrishna¬†et¬†al.¬†2019).  \n",
    "Feel free to:\n",
    "\n",
    "* switch to a **1‚ÄëD residual CNN**  \n",
    "* treat the spectrum as a **sentence** and use a Transformer  \n",
    "* replace pooling with **strided convolutions**\n",
    "\n",
    "### astroMiNN (Image¬†+¬†Metadata) ¬†\n",
    "A light‚Äëweight Mixture‚Äëof‚ÄëExperts inspired by *Mixture‚Äëof‚ÄëExperts for Multimodal Supernova Classification* (Caupin¬†et¬†al.,¬†2024).  \n",
    "Separate ‚Äútowers‚Äù encode PSF‚Äëshape, localisation, and contextual features, then a **fusion‚Äërouter** softly combines them.\n",
    "\n",
    "> **üí°¬†Try¬†This:** swap the Images tower for a pretrained ViT ‚Äúpatchified‚Äù stamp!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc683bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(1e4) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):  # (B,¬†T,¬†D)\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class PhotometryEncoder(nn.Module):\n",
    "    \"\"\"A CLS‚Äëtoken Transformer encoder for irregular light‚Äëcurves.\"\"\"\n",
    "    def __init__(self, input_dim=4):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(input_dim, cfg.d_model)\n",
    "        self.cls   = nn.Parameter(torch.zeros(1, 1, cfg.d_model))\n",
    "        self.pos   = PositionalEncoding(cfg.d_model, cfg.max_len + 1)\n",
    "        enc_layer  = nn.TransformerEncoderLayer(cfg.d_model, cfg.num_heads,\n",
    "                                                cfg.d_model * 4, cfg.dropout, batch_first=True)\n",
    "        self.enc   = nn.TransformerEncoder(enc_layer, cfg.num_layers)\n",
    "        self.norm  = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        B = x.size(0)\n",
    "        tok = self.cls.expand(B, -1, -1)\n",
    "        x   = torch.cat([tok, self.embed(x)], 1)\n",
    "        pad = torch.cat([torch.zeros(B, 1, dtype=torch.bool, device=x.device), pad_mask], 1)\n",
    "        h   = self.enc(self.pos(x), src_key_padding_mask=pad)\n",
    "        return self.norm(h[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d44bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectraEncoder(nn.Module):\n",
    "    \"\"\"DASH‚Äëstyle 2‚Äëlayer CNN over a (1,¬†59,¬†59) spectra image.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, cfg.conv1_filters, cfg.conv1_kernel,\n",
    "                               padding=cfg.conv1_kernel // 2)\n",
    "        self.bn1   = nn.BatchNorm2d(cfg.conv1_filters)\n",
    "        self.pool1 = nn.MaxPool2d(cfg.pool_size)\n",
    "        self.conv2 = nn.Conv2d(cfg.conv1_filters, cfg.conv2_filters, cfg.conv2_kernel,\n",
    "                               padding=cfg.conv2_kernel // 2)\n",
    "        self.bn2   = nn.BatchNorm2d(cfg.conv2_filters)\n",
    "        self.pool2 = nn.MaxPool2d(cfg.pool_size)\n",
    "        out_size   = cfg.grid_size // (cfg.pool_size * cfg.pool_size)\n",
    "        self.flat  = nn.Flatten()\n",
    "        self.fc1   = nn.Linear(cfg.conv2_filters * out_size * out_size, cfg.fc_units_spec)\n",
    "        self.drop  = nn.Dropout(cfg.fc_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x))); x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x))); x = self.pool2(x)\n",
    "        x = self.flat(x)\n",
    "        return self.drop(F.relu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f495659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import SqueezeExcitation\n",
    "from torchvision.models import DenseNet\n",
    "# NOTE: CoordCNNJointTower should be in the repo.\n",
    "from CNN import CoordCNNJointTower  # noqa\n",
    "\n",
    "class FeatureInteraction(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, dim * 2)\n",
    "        self.gate = nn.Sequential(nn.Linear(dim * 2, dim), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.gate(self.proj(x))\n",
    "\n",
    "class astroMiNN(nn.Module):\n",
    "    \"\"\"Image¬†+¬†metadata tower with MoE fusion and classification.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        H, P = cfg.astro_hidden_dim, cfg.astro_proj_dim\n",
    "\n",
    "        # --- per‚Äëfeature towers -------------------------------------------\n",
    "        self.psf_tower = nn.Sequential(\n",
    "            nn.Linear(2, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.spatial_tower = nn.Sequential(\n",
    "            nn.Linear(3, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.nst1_tower = nn.Sequential(\n",
    "            nn.Linear(2, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.nst2_tower = nn.Sequential(\n",
    "            nn.Linear(2, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.coord_cnn_tower = CoordCNNJointTower()  # image‚ÜíP\n",
    "\n",
    "        # --- fusion Mixture‚Äëof‚ÄëExperts ------------------------------------\n",
    "        self.fusion_experts = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(P * 5, 512), nn.LayerNorm(512),\n",
    "                          nn.GELU(), nn.Linear(512, P))\n",
    "            for _ in range(cfg.astro_num_fusion_experts)\n",
    "        ])\n",
    "        self.fusion_router = nn.Linear(P * 5, cfg.astro_num_fusion_experts)\n",
    "        self.proj_fuse     = nn.Sequential(nn.Linear(P, P), nn.LayerNorm(P), nn.GELU())\n",
    "\n",
    "        # --- classification Mixture‚Äëof‚ÄëExperts ----------------------------\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(P, P // 2), nn.LayerNorm(P // 2),\n",
    "                          nn.GELU(), nn.Dropout(.1),\n",
    "                          nn.Linear(P // 2, NUM_CLASSES))\n",
    "            for _ in range(cfg.astro_num_experts)\n",
    "        ])\n",
    "        self.router = nn.Linear(P, cfg.astro_num_experts)\n",
    "\n",
    "    def forward(self, x_meta, image):\n",
    "        psf_feats = self.psf_tower(x_meta[:, [5, 6]])           # sharpnr, scorr\n",
    "        spatial   = self.spatial_tower(x_meta[:, [2, 3, 4]])    # distpsnr 1/2 + nmtchps\n",
    "        nsta      = self.nst1_tower(x_meta[:, [0, 2]])          # sgscore1 + distpsnr1\n",
    "        nstb      = self.nst2_tower(x_meta[:, [1, 3]])          # sgscore2 + distpsnr2\n",
    "        joint     = self.coord_cnn_tower(x_meta[:, [7, 8]], image)\n",
    "\n",
    "        all_feats = torch.cat([nsta, nstb, spatial, psf_feats, joint], 1)\n",
    "        fusion_w  = F.softmax(self.fusion_router(all_feats), 1)\n",
    "        expert_out = torch.stack([e(all_feats) for e in self.fusion_experts], 1)\n",
    "        fused     = torch.einsum('be,bej->bj', fusion_w, expert_out)\n",
    "        fused     = self.proj_fuse(fused)\n",
    "\n",
    "        router_logits = self.router(fused)\n",
    "        expert_w      = F.softmax(router_logits, 1)\n",
    "        expert_logits = torch.stack([e(fused) for e in self.experts], 2)\n",
    "        return torch.einsum('be,bce->bc', expert_w, expert_logits)  # logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23eba32",
   "metadata": {},
   "source": [
    "## 5 ¬∑ Modal¬†Fusion \n",
    "\n",
    "The **simplest thing one could do** is to ignore all the fancy maths and just take the **plain average of the branch logits**, or even pick *‚Äúwhichever modality is most confident‚Äù* and run with it.  \n",
    "That baseline is fast and embarrassingly parallel, but it **throws away the chance for modalities to compensate for each other‚Äôs blind spots**‚Äîexactly what we need when spectra are noisy, photometry is sparse, or PSF stamps are half‚Äëclouded.\n",
    "\n",
    "Multimodal learning therefore revolves around **how** and **when** to mix information from different sources.  \n",
    "We distinguish three canonical strategies (Baltru≈°aitis‚ÄØ+‚ÄØAhuja‚ÄØ+‚ÄØMorency,¬†2019):\n",
    "\n",
    "| Strategy | Fusion equation | Pros | Cons |\n",
    "|----------|-----------------|------|------|\n",
    "| **Early fusion** | $ f_\\text{early}(x^{(1)},‚Ä¶,x^{(M)}) = g\\bigl([\\![ h_1(x^{(1)}) ; ‚Ä¶ ; h_M(x^{(M)}) ]\\!]\\bigr) $ | captures low‚Äëlevel cross‚Äëmodal cues, single end‚Äëto‚Äëend model | modality mis‚Äëalignment, huge input‚Äëdim |\n",
    "| **Intermediate fusion** | $ f_\\text{inter}(‚Ä¶) = g\\bigl(h_1(x^{(1)}), ‚Ä¶, h_M(x^{(M)})\\bigr) $ with cross‚Äëattention / co‚Äëtransformers | flexible, selective interaction | heavier, harder to debug |\n",
    "| **Late fusion** | $ f_\\text{late} = \\sum_{m=1}^{M} w_m \\, f_m(x^{(m)}) $ | simple, debuggable, modular | may miss fine cross‚Äëmodal nuance |\n",
    "\n",
    "Mathematically, if $ f_m: \\mathcal{X}^{(m)} \\to \\mathbb{R}^C $ outputs logits for each class, then\n",
    "\n",
    "$$\n",
    "\\hat{y}= \\operatorname*{arg\\,max}_c \n",
    "\\Bigl[\\,\n",
    "\\sum_{m=1}^{M} w_m \\, \\underbrace{f_m\\bigl(x^{(m)}\\bigr)}_{\\text{branch¬†logits}}\n",
    "\\Bigr]_c ,\n",
    "\\quad\n",
    "\\text{with } w_m \\ge 0,\\; \\sum_{m} w_m = 1.\n",
    "$$\n",
    "\n",
    "### Gated late‚Äëfusion (what we implement)\n",
    "\n",
    "Instead of fixed weights, we **learn** a *gate*:\n",
    "\n",
    "1. Stack branch logits: $ \\ell = [f_1 ; f_2 ; ‚Ä¶ ; f_M] ‚àà ‚Ñù^{M¬∑C} $  \n",
    "2. Pass through a small MLP:  \n",
    "   $ \\mathbf{w} = \\text{softmax}\\bigl(W_2 \\,\\sigma(W_1 \\ell)\\bigr) ‚àà ‚Ñù^{M} $  \n",
    "3. Final logits: $ \\tilde{f} = \\sum_{m} w_m f_m $\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* **Sample‚Äëadaptive** ‚Äì the model can down‚Äëweight noisy spectra or missing images.  \n",
    "* **Modular** ‚Äì you can hot‚Äëswap any branch without retraining the others (fine‚Äëtune gate).  \n",
    "* **Explainable** ‚Äì inspect $ \\mathbf{w} $ to see which modality ‚Äúwon‚Äù.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "* Gate only sees **logits**, not deep embeddings ‚Üí cannot exploit low‚Äëlevel synergy.  \n",
    "* Requires **all modalities at inference¬†time** (though you can zero‚Äëpad missing ones).\n",
    "\n",
    "> **üí°¬†Try¬†This:** Benchmark at least **two** fusion baselines against gated fusion:  \n",
    "> * **Uniform‚Äëaverage logits** ‚Äì $ \\tilde{f} = \\frac{1}{M}\\sum_m f_m $.  \n",
    "> * **Uniform‚Äëaverage encodings** ‚Äì average branch embeddings *before* the classification head.  \n",
    "> * **Concatenate‚Äëthen‚ÄëMLP** ‚Äì stack embeddings and learn one big predictor (classic early fusion).  \n",
    "> * **Confidence‚Äëweighted average** ‚Äì weight logits by their inverse entropy.  \n",
    "> * **Cross-Attention Fusion** ‚Äì  \n",
    "> Report BALANCED ACCURACY and plot how fusion weights evolve over training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7072ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalClassifier(nn.Module):\n",
    "    \"\"\"Three branches ‚Üí gated late‚Äëfusion.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # phot + spectra\n",
    "        self.phot = PhotometryEncoder()\n",
    "        self.spec = SpectraEncoder()\n",
    "        fusion_dim = cfg.d_model + cfg.fc_units_spec\n",
    "        self.head  = nn.Sequential(nn.Linear(fusion_dim, 512),\n",
    "                                   nn.ReLU(), nn.Dropout(0.3),\n",
    "                                   nn.Linear(512, NUM_CLASSES))\n",
    "\n",
    "        # astroMiNN\n",
    "        self.astro = astroMiNN()\n",
    "\n",
    "        # gated fusion\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(NUM_CLASSES * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, phot, mask, spec, meta, img):\n",
    "        h_phot = self.phot(phot, mask)\n",
    "        h_spec = self.spec(spec)\n",
    "        logits_ps = self.head(torch.cat([h_phot, h_spec], 1))\n",
    "        logits_im = self.astro(meta, img)\n",
    "        stacked   = torch.cat([logits_ps, logits_im], 1)\n",
    "        w = torch.softmax(self.gate(stacked), 1)\n",
    "        final_logits = w[:, 0:1] * logits_ps + w[:, 1:2] * logits_im\n",
    "        return final_logits\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Œ≥‚Äëweighted focal loss with optional Œ± balancing.\"\"\"\n",
    "    def __init__(self, gamma=cfg.gamma, alpha=None):\n",
    "        super().__init__()\n",
    "        self.g, self.a = gamma, alpha\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        logp = F.log_softmax(logits, 1); p = logp.exp()\n",
    "        idx  = torch.arange(logits.size(0), device=logits.device)\n",
    "        logp_t, p_t = logp[idx, target], p[idx, target]\n",
    "        loss = -((1 - p_t) ** self.g) * logp_t\n",
    "        if self.a is not None:\n",
    "            loss *= self.a[target]\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81d42c",
   "metadata": {},
   "source": [
    "## 6 ¬∑ Data Preparation\n",
    "\n",
    "Pipeline summary:\n",
    "\n",
    "1. **File sanity check:** drop rows whose `.npy` bundle is missing.  \n",
    "2. **Random 80‚ÄØ% /¬†20‚ÄØ% split** for train/val (*stratified* would be nicer‚Äîexercise!).  \n",
    "3. **Spectra StandardScaler** fitted on training set only.  \n",
    "4. **Photometry channel mean‚ÄØ¬±‚ÄØstd** for z‚Äëscore normalisation.  \n",
    "5. **Class‚Äëbalanced Œ±** for focal‚Äëloss: \\( \\alpha_c ‚àù 1/\\sqrt{N_c} \\).\n",
    "\n",
    "> **üí°¬†Try¬†This:** instead of `StandardScaler`, fit a **RobustScaler** to down‚Äëweight spectral outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727936a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1¬†Load CSV and sanity‚Äëcheck files\n",
    "exist_mask = full_df['file'].apply(lambda f: (Path(cfg.root_path) / f).exists())\n",
    "missing = (~exist_mask).sum()\n",
    "if missing:\n",
    "    warnings.warn(f\"dropping {missing} samples with missing .npy\")\n",
    "full_df = full_df[exist_mask].reset_index(drop=True)\n",
    "\n",
    "# 6.2¬†Train/val split --------------------------------------------------------\n",
    "train_df, val_df = np.split(full_df.sample(frac=1, random_state=cfg.seed),\n",
    "                            [int(0.8 * len(full_df))])\n",
    "\n",
    "# 6.3¬†Spectra scaler ---------------------------------------------------------\n",
    "spec_scaler = StandardScaler()\n",
    "for f in train_df['file']:\n",
    "    arr = np.nan_to_num(np.load(Path(cfg.root_path) / f, allow_pickle=True).item()['spectra'][0])\n",
    "    spec_scaler.partial_fit(arr.reshape(1, -1))\n",
    "\n",
    "# 6.4¬†Datasets & loaders -----------------------------------------------------\n",
    "train_ds = MultiModalDataset(train_df, cfg.root_path, spec_scaler=spec_scaler,\n",
    "                             grid_size=cfg.grid_size)\n",
    "phot_mean, phot_std = compute_phot_mean_std(train_ds)\n",
    "train_ds.phot_mean, train_ds.phot_std = phot_mean, phot_std\n",
    "\n",
    "val_ds = MultiModalDataset(val_df, cfg.root_path, phot_mean, phot_std,\n",
    "                           spec_scaler, cfg.grid_size)\n",
    "\n",
    "collate = build_collate(phot_mean, phot_std)\n",
    "train_ld = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                      num_workers=cfg.num_workers, collate_fn=collate, drop_last=True)\n",
    "val_ld   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                      num_workers=cfg.num_workers, collate_fn=collate)\n",
    "\n",
    "# 6.5¬†Œ±¬†balancing for focal loss --------------------------------------------\n",
    "labels_train = torch.tensor([train_ds[i][-1] for i in range(len(train_ds))])\n",
    "counts = torch.bincount(labels_train, minlength=NUM_CLASSES).float()\n",
    "alpha  = (1.0 / torch.sqrt(counts)).to(device)\n",
    "\n",
    "print('‚úîÔ∏è  Data ready. Train batches:', len(train_ld), '| Val batches:', len(val_ld))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354a265",
   "metadata": {},
   "source": [
    "## 7 ¬∑ Training Loop\n",
    "\n",
    "We use:\n",
    "\n",
    "* **AdamW** optimiser  \n",
    "* **Cosine‚ÄëAnnealing‚ÄëWarm‚ÄëRestarts** LR schedule  \n",
    "* **Focal loss** with Œ≥‚ÄØ=‚ÄØ2 to fight class imbalance  \n",
    "* **Gradient clipping** at 1.0¬†norm  \n",
    "* **Early stopping** on *macro AUPRC* with patience‚ÄØ=‚ÄØ50 epochs\n",
    "\n",
    "Keep an eye on **Weights¬†&¬†Biases** dashboards: learning curves, ROC, PR, and gate weights per batch.\n",
    "\n",
    "> **üí°¬†Try¬†This:** try early stopping on other criteria, such as Balanced Accuracy or Validation Loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=cfg.project, mode=cfg.wandb_mode, config=vars(cfg))\n",
    "\n",
    "model = MultiModalClassifier().to(device)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "crit  = FocalLoss(cfg.gamma, alpha)\n",
    "\n",
    "best_auprc, no_imp = 0.0, 0\n",
    "ckpt_path = Path(wandb.run.dir) / 'best.pt'\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    tl, correct, N = 0., 0, 0\n",
    "    all_prob, all_lab = [], []\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for phot, mask, spec, meta, img, y in loader:\n",
    "            phot, mask, spec = phot.to(device), mask.to(device), spec.to(device)\n",
    "            meta, img, y     = meta.to(device), img.to(device), y.to(device)\n",
    "\n",
    "            logits = model(phot, mask, spec, meta, img)\n",
    "            loss   = crit(logits, y)\n",
    "\n",
    "            if train:\n",
    "                opt.zero_grad(); loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                opt.step(); sched.step()\n",
    "\n",
    "            tl += loss.item() * y.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "            N += y.size(0)\n",
    "            all_prob.append(F.softmax(logits, 1).detach().cpu())\n",
    "            all_lab.append(y.cpu())\n",
    "\n",
    "    probs = torch.cat(all_prob); labs = torch.cat(all_lab)\n",
    "    acc   = correct / N\n",
    "    auroc = roc_auc_score(labs, probs, multi_class='ovr', average='macro')\n",
    "    return tl / N, acc, auroc, labs.numpy(), probs.numpy()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "for ep in range(1, cfg.epochs + 1):\n",
    "    tr_loss, tr_acc, tr_auc, *_ = run_epoch(train_ld, True)\n",
    "    va_loss, va_acc, va_auc, yt, yp = run_epoch(val_ld, False)\n",
    "\n",
    "    y_onehot = label_binarize(yt, classes=np.arange(NUM_CLASSES))\n",
    "    va_auprc = average_precision_score(y_onehot, yp, average='macro')\n",
    "\n",
    "    wandb.log({'epoch': ep,\n",
    "               'train/loss': tr_loss, 'train/acc': tr_acc,\n",
    "               'val/loss': va_loss, 'val/acc': va_acc,\n",
    "               'val/auroc_macro': va_auc, 'val/auprc_macro': va_auprc,\n",
    "               'lr': sched.get_last_lr()[0]})\n",
    "\n",
    "    if va_auprc > best_auprc:\n",
    "        best_auprc, no_imp = va_auprc, 0\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        wandb.save(str(ckpt_path))\n",
    "    else:\n",
    "        no_imp += 1\n",
    "        if no_imp >= cfg.patience:\n",
    "            print('‚èπ Early stopping'); break\n",
    "    print(f'[ep {ep:03d}] tr_acc={tr_acc:.3f}  va_acc={va_acc:.3f}  va_AUPRC={va_auprc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c8717",
   "metadata": {},
   "source": [
    "## 8 ¬∑ Evaluation & Visualisation\n",
    "\n",
    "For each split we log:\n",
    "\n",
    "* *Metrics*: Acc, F1‚Äëmacro, AUROC‚Äëmacro, AUPRC‚Äëmacro, Top‚Äë2/3 accuracy  \n",
    "* *Plots*: ROC curve per class, PR curve, confusion matrix, confidence histograms  \n",
    "\n",
    "Interpretation tips:\n",
    "\n",
    "* **ROC vs¬†PR:** PR is more sensitive to class imbalance‚Äîoptimise AUPRC here.  \n",
    "* **Confidence histogram:** a well‚Äëcalibrated classifier shows sharp peaks near‚ÄØ0 and‚ÄØ1 for each class.  \n",
    "* **Gate weights:** if $ w_\\text{phot} $ is systematically low, maybe the photometry encoder is under‚Äëtrained. That is very common.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a08698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, split):\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    loss, acc, auc_macro, y, p = run_epoch(loader, False)\n",
    "    y_pred = p.argmax(1)\n",
    "    # Your code here...\n",
    "\n",
    "val_metrics = evaluate(val_ld, 'val')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278241ba-031e-41d0-8134-2e50df89d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1¬†Test set ---------------------------------------------------------------\n",
    "\n",
    "## We will do this later!\n",
    "\n",
    "\n",
    "#test_df = pd.read_csv(cfg.test_csv_path)\n",
    "#test_df['type'] = test_df['type'].map(ORIG2BROAD)\n",
    "#test_ds = MultiModalDataset(test_df, cfg.test_root, phot_mean, phot_std,\n",
    "                            spec_scaler, cfg.grid_size)\n",
    "#test_ld = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                     #num_workers=cfg.num_workers, collate_fn=collate)\n",
    "#test_metrics = evaluate(test_ld, 'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4b108",
   "metadata": {},
   "source": [
    "## 9 ¬∑ Next Steps & Stretch¬†Goals \n",
    "\n",
    "| Goal | Suggestion |\n",
    "|------|------------|\n",
    "| **Better Encoders** | Try different encoders for all modalities. Some times, less can be more! |\n",
    "| **Better Fusion** | No wrong answers here. Be creative! Talking about that... |\n",
    "| **Early‚Äëfusion Transformer** | Concatenate photometry fluxes, binned spectra, and PSF embeddings into one mega‚Äësequence; add modality tokens. Take a look at *Perceiver I/O*! |\n",
    "| **Full taxonomy** | Expand `ORIG2BROAD` mapping to all subclasses; swap loss to class‚Äëbalanced focal. |\n",
    "| **Early‚Äëfusion Transformer** | Concatenate photometry fluxes, binned spectra, and PSF embeddings into one mega‚Äësequence; add modality tokens. |\n",
    "| **Co‚Äëattention (Inter‚Äëfusion)** | Implement *LXMERT*‚Äëstyle cross‚Äëattention between phot and spec encoders. |\n",
    "| **Self‚Äësupervised pre‚Äëtraining** | Masked‚Äëvalue reconstruction on light‚Äëcurves; BYOL on PSF stamps. |\n",
    "| **Uncertainty‚Äëaware gating** | Use logits‚Äô variance or entropy to re‚Äëweight unreliable modalities. |\n",
    "| **Hyper‚Äëparam sweep** | `wandb.sweep` on LR, Œ≥, #heads, experts. |\n",
    "| **Model ensembling** | Logit‚Äëaverage 5 checkpoints with different seeds. |\n",
    "\n",
    "\n",
    "**Leaderboard metric:** *BALANCED ACCURACY on the hidden test‚Äëset.*  \n",
    "Push your best run to Slack Day05, with some beautiful plots :)\n",
    "\n",
    "Happy hacking!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc91668-50bd-4a9a-b309-39dc6a8f17f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nmma_env)",
   "language": "python",
   "name": "nmma_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
