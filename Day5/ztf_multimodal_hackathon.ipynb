{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f5bfef",
   "metadata": {},
   "source": [
    "# Multimodal Early‑Classification of ZTF Transients  \n",
    "### *A mini “AppleCiDEr” hackathon*\n",
    "\n",
    "\n",
    "### Get the data here! https://drive.google.com/drive/u/0/folders/19EUnPU4AyN8Y0dF_CYsNHHxz94qjL1q9 \n",
    "\n",
    "You should download all the data_train_multi files.\n",
    "\n",
    "\n",
    "\n",
    "Welcome to the **grand‑finale hackathon** of the 2025 ZTF Summer School!  \n",
    "In the next few hours you will build, train, and evaluate a **multimodal classifier** that decides—within the **first 10 days** of discovery—what kind of optical transient we are watching.\n",
    "\n",
    "Why does *early* classification matter?\n",
    "\n",
    "* *Rapid follow‑up:* scarce resources (Spectrographs, JWST, HST, VLA…) must be allocated **before** a transient fades.  \n",
    "* *Fast transients physics:* the more data we have, the better we can constrain models.  \n",
    "* *Event brokers:* surveys such as ZTF, Rubin, ... stream millions of alerts/night; automated early triage is critical!\n",
    "\n",
    "We will focus our taxonomy into **five broad classes** to keep training fast for the time scale of the hackathon:\n",
    "\n",
    "| Broad class | Typical zoo members |\n",
    "|-------------|--------------------|\n",
    "| **SNI**  | SN Ia, Ib, Ic |\n",
    "| **SNII** | SN II, II‑P, IIn, II‑b |\n",
    "| **CV** | Cataclysmic Variables |\n",
    "| **AGN** | Active Galactic Nuclei |\n",
    "| **TDE** |  Tidal Disruption Events |\n",
    "\n",
    "In spirit we are reproducing a toy version of  \n",
    "> **“Applying Multimodal Learning to Classify Transient Detections Early (AppleCiDEr) I: Dataset, Methods, and Infrastructure”** – *AJ Brown et al., 2025*.\n",
    "\n",
    "Good luck and have fun!\n",
    "\n",
    "As a starting point, we are providing an early development version of AppleCider. The cells are supposed to be more of a guide then a fully functional code; There will be places where you will either have to complete available code or write your own. Feel free to transfer the parts relevant to you to your own coding environment.\n",
    "\n",
    "This version of the **multimodal classifier** combines:\n",
    "\n",
    "| Modality | Encoder | Key Idea |\n",
    "|----------|---------|----------|\n",
    "| **Photometry** | Transformer | temporal self‑attention on light‑curves |\n",
    "| **Spectra** | CNN | harvesting power from 2D CNNs |\n",
    "| **Image + Metadata** | astroMiNN | Mixture‑of‑Experts fusion tower |\n",
    "| **Fusion** | Gated late‑fusion | sample‑adaptive weighting of branch logits |\n",
    "  \n",
    "Everything is thoroughly annotated, but if you have any questions about any of the bits, please let us know :) \n",
    "\n",
    "> **Your challenge:** improve BALANCED ACCURACY of the multi-modal classifier. Feel free to exercise all of your creativity and substitute any of the blocks in this code, to implement any technique you like. We left some tips that might be worth exploring on the “💡 Try This” cells sprinkled throughout.\n",
    ">\n",
    "> Happy Hacking!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5a89b",
   "metadata": {},
   "source": [
    "## 0 · Setup & Dependencies\n",
    "\n",
    "- We suggest making a fresh conda environment for this challenge.  \n",
    "- If you run locally, uncomment the cell below to install some of the important Python packages.\n",
    "\n",
    "Feel free to add any extra libraries you need (like `tqdm`, `rich`, `torchmetrics`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72029ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install numpy pandas scikit-learn seaborn wandb tqdm rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f06909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, argparse, warnings, json, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, f1_score,\n",
    "                             precision_score, recall_score, average_precision_score,\n",
    "                             confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
    "                             top_k_accuracy_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "# Pretty plots\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "plt.rcParams['figure.autolayout'] = True\n",
    "wandb.require(\"service\")\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0061528",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit the **data paths** below if you stashed the CSVs or `.npy` files in a different directory.  \n",
    "All other hyper‑parameters are sane defaults, but you are encouraged to play:\n",
    "\n",
    "* Transformer depth, heads, dropout  \n",
    "* CNN kernel sizes / channels  \n",
    "* Number of experts in astroMiNN  \n",
    "* Learning rate schedule, batch size, $\\gamma$ for focal‑loss\n",
    "\n",
    "> **💡 Try This:** crank `d_model` to 256 and halve `epochs` while using cosine restarts; Sometimes, bigger models converge faster, but can be less stable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b09c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # ── data paths (EDIT ME) ────────────────────────────────────────────────\n",
    "    csv_path      = \".../data_train_multi_10.csv\"\n",
    "    root_path     = \".../data_train_multi/day10\"\n",
    "    test_csv_path = \".../data_test_multi_10.csv\"\n",
    "    test_root     = \"/data_test_multi/day10\"\n",
    "\n",
    "    # ── experiment tracking ────────────────────────────────────────────────\n",
    "    project       = \"multimodal_late_fusion_gated\"\n",
    "    wandb_mode    = \"disabled\"         # 'disabled' for offline\n",
    "\n",
    "    # ── optimisation ───────────────────────────────────────────────────────\n",
    "    batch_size    = 128\n",
    "    epochs        = 150\n",
    "    lr            = 1e-6\n",
    "    weight_decay  = 1e-2\n",
    "    patience      = 50\n",
    "    num_workers   = 10\n",
    "    gamma         = 2.0              # focal‑loss γ\n",
    "    seed          = 42\n",
    "\n",
    "    # ── photometry Transformer ─────────────────────────────────────────────\n",
    "    d_model       = 128\n",
    "    num_layers    = 8\n",
    "    num_heads     = 4\n",
    "    dropout       = 0.3\n",
    "    max_len       = 500\n",
    "\n",
    "    # ── spectra CNN ────────────────────────────────────────────────────────\n",
    "    grid_size           = 59\n",
    "    conv1_filters       = 32\n",
    "    conv1_kernel        = 7\n",
    "    conv2_filters       = 64\n",
    "    conv2_kernel        = 7\n",
    "    pool_size           = 2\n",
    "    fc_units_spec       = 2048\n",
    "    fc_dropout          = 0.3\n",
    "\n",
    "    # ── astroMiNN (image + metadata) ───────────────────────────────────────\n",
    "    astro_input_dim           = 9\n",
    "    astro_hidden_dim          = 128\n",
    "    astro_proj_dim            = 256\n",
    "    astro_num_experts         = 10\n",
    "    astro_num_fusion_experts  = 5\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# Determinism helpers\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "random.seed(cfg.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── taxonomy ---------------------------------------------------------------\n",
    "BROAD_CLASSES = [\"SNI\", \"SNII\", \"Cataclysmic\", \"AGN\", \"TDE\"]\n",
    "NUM_CLASSES   = len(BROAD_CLASSES)\n",
    "ORIG2BROAD = {\n",
    "    \"SN Ia\":\"SNI\",\"SN Ib\":\"SNI\",\"SN Ic\":\"SNI\",\n",
    "    \"SN II\":\"SNII\",\"SN IIP\":\"SNII\",\"SN IIn\":\"SNII\",\"SN IIb\":\"SNII\",\n",
    "    \"Cataclysmic\":\"Cataclysmic\",\"AGN\":\"AGN\",\"Tidal Disruption Event\":\"TDE\"\n",
    "}\n",
    "BROAD2ID = {c:i for i,c in enumerate(BROAD_CLASSES)}\n",
    "\n",
    "# ── pre‑computed metadata normalisation stats ------------------------------\n",
    "METADATA_STATS = {\n",
    "    'sgscore1': {'mean': 0.236, 'std': 0.266},\n",
    "    'sgscore2': {'mean': 0.401, 'std': 0.328},\n",
    "    'distpsnr1': {'mean': 3.151, 'std': 3.757},\n",
    "    'distpsnr2': {'mean': 9.269, 'std': 6.323},\n",
    "    'nmtchps': {'mean': 9.231, 'std': 8.089},\n",
    "    'sharpnr': {'mean': 0.253, 'std': 0.256},\n",
    "    'scorr': {'mean': 22.089, 'std': 16.757},\n",
    "    'ra': {'mean': 0, 'std': 1},\n",
    "    'dec': {'mean': 0, 'std': 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc3503",
   "metadata": {},
   "source": [
    "## 2 · Data Exploration\n",
    "\n",
    "\n",
    "Before touching any code, let’s *look* at the dataset. Your first exercise is to obtain as much information from the dataset as you can by plotting data for individual objects, statistics of relevant features...\n",
    "\n",
    "* **Rows:** individual ZTF objects (one per transient)  \n",
    "* **file:** `.npy` bundle containing the four modalities  \n",
    "* **type:** label map \n",
    "\n",
    "As a first suggestion, let’s peek at the CSV that indexes our multimodal samples and visualise the class distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(cfg.csv_path)\n",
    "full_df['type'] = full_df['type'].map(ORIG2BROAD)\n",
    "display(full_df.head())\n",
    "\n",
    "print(f\"Total samples: {len(full_df):,}\")\n",
    "class_counts = full_df['type'].value_counts().reindex(BROAD_CLASSES).fillna(0)\n",
    "class_counts.plot(kind='bar', rot=0)\n",
    "plt.title('Class Distribution'); plt.ylabel('# objects'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a8ac3-76b4-4456-a98d-d683f1c03810",
   "metadata": {},
   "source": [
    "> **💡 Try This:** Ask yourself whether this level of class imbalance will significantly affect your training. If yes, what techniques could you employ to fight it? Oversampling the minority classes or Undersampling the majority classes are two common techniques, but there are many others out there!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00396db2",
   "metadata": {},
   "source": [
    "## 3 · Dataset & Dataloaders\n",
    "\n",
    "### Anatomy of one sample\n",
    "\n",
    "| Tensor | Shape | Meaning |\n",
    "|--------|-------|---------|\n",
    "| `phot` | `(T, 4)` | ZTF light‑curve (g/r flux, flux‑err, mjd) |\n",
    "| `spec_img` | `(1, 59, 59)` | Raw spectrum ▸ z‑score ▸ square grid |\n",
    "| `meta_vec` | `(9,)` | Contextual features (sgscore, distpsnr, …) |\n",
    "| `image` | `(3, H, W)` | Triplet of Image Cutouts, scaled to [0, 1] |\n",
    "| `label` | scalar | 0–4 for the 5 broad classes |\n",
    "\n",
    "Key implementation tricks:\n",
    "\n",
    "1. Padding + masking for variable‑length photometry sequences so the Transformer ignores padded tokens.  \n",
    "2. StandardScaler on spectra voxels—fitted on *training* set only (very important, to avoid leakage!).  \n",
    "3. On‑the‑fly square‑grid mapping so you can easily swap‑in different `grid_size` or even 1‑D CNNs.\n",
    "\n",
    "> **💡 Try This:** If preprocessing the spectra is taking too much performance out of your dataloader, cache the mapped spectra on disk for a decent speed‑up at the cost of some RAM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset(Dataset):\n",
    "    \"\"\"Custom dataset returning five tensors per sample.\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, root: Path,\n",
    "                 phot_mean=None, phot_std=None, spec_scaler=None,\n",
    "                 grid_size=cfg.grid_size, transform=None):\n",
    "        self.df   = df.reset_index(drop=True)\n",
    "        self.root = Path(root)\n",
    "        self.phot_mean = phot_mean\n",
    "        self.phot_std  = phot_std\n",
    "        self.spec_scaler = spec_scaler\n",
    "        self.grid_size   = grid_size\n",
    "        self.transform   = transform\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def _load_sample(self, row):\n",
    "        sample = np.load(self.root / row['file'], allow_pickle=True).item()\n",
    "        phot   = torch.tensor(sample['photometry'], dtype=torch.float32)\n",
    "        spec   = np.nan_to_num(sample['spectra'][0])\n",
    "\n",
    "        # --- metadata vector (z‑score or min‑max) ----------\n",
    "        m = sample['metadata']\n",
    "        meta_vec = torch.tensor([\n",
    "            (m['sgscore1']  - METADATA_STATS['sgscore1']['mean']) / METADATA_STATS['sgscore1']['std'],\n",
    "            (m['sgscore2']  - METADATA_STATS['sgscore2']['mean']) / METADATA_STATS['sgscore2']['std'],\n",
    "            (m['distpsnr1'] - METADATA_STATS['distpsnr1']['mean']) / METADATA_STATS['distpsnr1']['std'],\n",
    "            (m['distpsnr2'] - METADATA_STATS['distpsnr2']['mean']) / METADATA_STATS['distpsnr2']['std'],\n",
    "            (m['nmtchps']   - METADATA_STATS['nmtchps']['mean'])  / METADATA_STATS['nmtchps']['std'],\n",
    "            (m['sharpnr']   - METADATA_STATS['sharpnr']['mean'])  / METADATA_STATS['sharpnr']['std'],\n",
    "            (m['scorr']     - METADATA_STATS['scorr']['mean'])    / METADATA_STATS['scorr']['std'],\n",
    "            (m['ra'] / 360.0),\n",
    "            ((m['dec'] + 90.0) / 180.0)\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        # --- PSF stamp -------------------------------------\n",
    "        img = torch.tensor(sample['images'], dtype=torch.float32).permute(2,0,1) / 255.0\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return phot, spec, meta_vec, img\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phot, spec, meta_vec, img = self._load_sample(row)\n",
    "\n",
    "        # spectra → square grid\n",
    "        spec_std = self.spec_scaler.transform([spec])[0]\n",
    "        needed   = self.grid_size ** 2\n",
    "        if spec_std.shape[0] < needed:\n",
    "            spec_std = np.pad(spec_std, (0, needed - spec_std.shape[0]))\n",
    "        elif spec_std.shape[0] > needed:\n",
    "            spec_std = spec_std[:needed]\n",
    "        spec_img = torch.tensor(spec_std.reshape(self.grid_size, self.grid_size),\n",
    "                                dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label = BROAD2ID[row['type']]\n",
    "        return phot, spec_img, meta_vec, img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phot_mean_std(ds):\n",
    "    \"\"\"Per‑channel mean/std over an *un‑padded* photometry dataset.\"\"\"\n",
    "    sums, sqs, n = torch.zeros(4), torch.zeros(4), 0\n",
    "    for phot, *_ in ds:\n",
    "        sums += phot.sum(0)\n",
    "        sqs  += (phot ** 2).sum(0)\n",
    "        n    += phot.shape[0]\n",
    "    mean = (sums / n)[None, None, :]\n",
    "    std  = torch.sqrt(sqs / n - mean.squeeze() ** 2)[None, None, :]\n",
    "    return mean, std\n",
    "\n",
    "def build_collate(mean, std):\n",
    "    def collate(batch):\n",
    "        phot_seqs, spec_imgs, meta_vecs, img_tensors, labels = zip(*batch)\n",
    "\n",
    "        spec_batch = torch.stack(spec_imgs)\n",
    "        meta_batch = torch.stack(meta_vecs)\n",
    "        img_batch  = torch.stack(img_tensors)\n",
    "\n",
    "        lengths = [p.shape[0] for p in phot_seqs]\n",
    "        padded  = pad_sequence(phot_seqs, batch_first=True)\n",
    "        mask    = torch.stack([\n",
    "            torch.cat([torch.zeros(l), torch.ones(padded.shape[1]-l)])\n",
    "            for l in lengths\n",
    "        ]).bool()\n",
    "        normed  = (padded - mean) / (std + 1e-8)\n",
    "        return normed, mask, spec_batch, meta_batch, img_batch, torch.tensor(labels)\n",
    "    return collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afacbe5",
   "metadata": {},
   "source": [
    "## 4 · Modal Encoders\n",
    "\n",
    "### Photometry Transformer  \n",
    "Temporal self‑attention excels at capturing irregular cadence and long‑range dependencies.  \n",
    "We prepend a learnable **[CLS] token** whose output embedding summarises the entire light‑curve.\n",
    "\n",
    "> **💡 Try This:** Can just the [CLS] token hold all the information about the transient classes or would be better to use all output encondings from the Transformer model? \n",
    "\n",
    "\n",
    "> **💡 Try This:** Can we include a better Positional Encoding, that doesn't rely on the index of the photometric point, but actually encodes the timing between observations? Learnable, time-aware positional encoding was shown to do better in time-series. \n",
    "\n",
    "*Input*: padded sequence `x = (time, g_flux, r_flux, i_flux) ∈ ℝ^{B×T×4}`.   \n",
    "*PosEnc*: $ \\text{PE}(t,2i)=\\sin(t/10000^{2i/d}) $, $ \\text{PE}(t,2i+1)=\\cos(t/10000^{2i/d}) $\n",
    "\n",
    "### Spectra CNN  \n",
    "We reshape each 1‑D spectrum into a 2‑D postage‑stamp and apply two convolutional blocks. Cool reference about Spectrum Classification: *DASH* (Muthukrishna et al. 2019).  \n",
    "Feel free to:\n",
    "\n",
    "* switch to a **1‑D residual CNN**  \n",
    "* treat the spectrum as a **sentence** and use a Transformer  \n",
    "* replace pooling with **strided convolutions**\n",
    "\n",
    "### astroMiNN (Image + Metadata)  \n",
    "A light‑weight Mixture‑of‑Experts inspired by *Mixture‑of‑Experts for Multimodal Supernova Classification* (Caupin et al., 2024).  \n",
    "Separate “towers” encode PSF‑shape, localisation, and contextual features, then a **fusion‑router** softly combines them.\n",
    "\n",
    "> **💡 Try This:** swap the Images tower for a pretrained ViT “patchified” stamp!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc683bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(1e4) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):  # (B, T, D)\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class PhotometryEncoder(nn.Module):\n",
    "    \"\"\"A CLS‑token Transformer encoder for irregular light‑curves.\"\"\"\n",
    "    def __init__(self, input_dim=4):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(input_dim, cfg.d_model)\n",
    "        self.cls   = nn.Parameter(torch.zeros(1, 1, cfg.d_model))\n",
    "        self.pos   = PositionalEncoding(cfg.d_model, cfg.max_len + 1)\n",
    "        enc_layer  = nn.TransformerEncoderLayer(cfg.d_model, cfg.num_heads,\n",
    "                                                cfg.d_model * 4, cfg.dropout, batch_first=True)\n",
    "        self.enc   = nn.TransformerEncoder(enc_layer, cfg.num_layers)\n",
    "        self.norm  = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        B = x.size(0)\n",
    "        tok = self.cls.expand(B, -1, -1)\n",
    "        x   = torch.cat([tok, self.embed(x)], 1)\n",
    "        pad = torch.cat([torch.zeros(B, 1, dtype=torch.bool, device=x.device), pad_mask], 1)\n",
    "        h   = self.enc(self.pos(x), src_key_padding_mask=pad)\n",
    "        return self.norm(h[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d44bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectraEncoder(nn.Module):\n",
    "    \"\"\"DASH‑style 2‑layer CNN over a (1, 59, 59) spectra image.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, cfg.conv1_filters, cfg.conv1_kernel,\n",
    "                               padding=cfg.conv1_kernel // 2)\n",
    "        self.bn1   = nn.BatchNorm2d(cfg.conv1_filters)\n",
    "        self.pool1 = nn.MaxPool2d(cfg.pool_size)\n",
    "        self.conv2 = nn.Conv2d(cfg.conv1_filters, cfg.conv2_filters, cfg.conv2_kernel,\n",
    "                               padding=cfg.conv2_kernel // 2)\n",
    "        self.bn2   = nn.BatchNorm2d(cfg.conv2_filters)\n",
    "        self.pool2 = nn.MaxPool2d(cfg.pool_size)\n",
    "        out_size   = cfg.grid_size // (cfg.pool_size * cfg.pool_size)\n",
    "        self.flat  = nn.Flatten()\n",
    "        self.fc1   = nn.Linear(cfg.conv2_filters * out_size * out_size, cfg.fc_units_spec)\n",
    "        self.drop  = nn.Dropout(cfg.fc_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x))); x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x))); x = self.pool2(x)\n",
    "        x = self.flat(x)\n",
    "        return self.drop(F.relu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f495659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import SqueezeExcitation\n",
    "from torchvision.models import DenseNet\n",
    "# NOTE: CoordCNNJointTower should be in the repo.\n",
    "from CNN import CoordCNNJointTower  # noqa\n",
    "\n",
    "class FeatureInteraction(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, dim * 2)\n",
    "        self.gate = nn.Sequential(nn.Linear(dim * 2, dim), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.gate(self.proj(x))\n",
    "\n",
    "class astroMiNN(nn.Module):\n",
    "    \"\"\"Image + metadata tower with MoE fusion and classification.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        H, P = cfg.astro_hidden_dim, cfg.astro_proj_dim\n",
    "\n",
    "        # --- per‑feature towers -------------------------------------------\n",
    "        self.psf_tower = nn.Sequential(\n",
    "            nn.Linear(2, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.spatial_tower = nn.Sequential(\n",
    "            nn.Linear(3, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.nst1_tower = nn.Sequential(\n",
    "            nn.Linear(2, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.nst2_tower = nn.Sequential(\n",
    "            nn.Linear(2, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.coord_cnn_tower = CoordCNNJointTower()  # image→P\n",
    "\n",
    "        # --- fusion Mixture‑of‑Experts ------------------------------------\n",
    "        self.fusion_experts = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(P * 5, 512), nn.LayerNorm(512),\n",
    "                          nn.GELU(), nn.Linear(512, P))\n",
    "            for _ in range(cfg.astro_num_fusion_experts)\n",
    "        ])\n",
    "        self.fusion_router = nn.Linear(P * 5, cfg.astro_num_fusion_experts)\n",
    "        self.proj_fuse     = nn.Sequential(nn.Linear(P, P), nn.LayerNorm(P), nn.GELU())\n",
    "\n",
    "        # --- classification Mixture‑of‑Experts ----------------------------\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(P, P // 2), nn.LayerNorm(P // 2),\n",
    "                          nn.GELU(), nn.Dropout(.1),\n",
    "                          nn.Linear(P // 2, NUM_CLASSES))\n",
    "            for _ in range(cfg.astro_num_experts)\n",
    "        ])\n",
    "        self.router = nn.Linear(P, cfg.astro_num_experts)\n",
    "\n",
    "    def forward(self, x_meta, image):\n",
    "        psf_feats = self.psf_tower(x_meta[:, [5, 6]])           # sharpnr, scorr\n",
    "        spatial   = self.spatial_tower(x_meta[:, [2, 3, 4]])    # distpsnr 1/2 + nmtchps\n",
    "        nsta      = self.nst1_tower(x_meta[:, [0, 2]])          # sgscore1 + distpsnr1\n",
    "        nstb      = self.nst2_tower(x_meta[:, [1, 3]])          # sgscore2 + distpsnr2\n",
    "        joint     = self.coord_cnn_tower(x_meta[:, [7, 8]], image)\n",
    "\n",
    "        all_feats = torch.cat([nsta, nstb, spatial, psf_feats, joint], 1)\n",
    "        fusion_w  = F.softmax(self.fusion_router(all_feats), 1)\n",
    "        expert_out = torch.stack([e(all_feats) for e in self.fusion_experts], 1)\n",
    "        fused     = torch.einsum('be,bej->bj', fusion_w, expert_out)\n",
    "        fused     = self.proj_fuse(fused)\n",
    "\n",
    "        router_logits = self.router(fused)\n",
    "        expert_w      = F.softmax(router_logits, 1)\n",
    "        expert_logits = torch.stack([e(fused) for e in self.experts], 2)\n",
    "        return torch.einsum('be,bce->bc', expert_w, expert_logits)  # logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23eba32",
   "metadata": {},
   "source": [
    "## 5 · Modal Fusion \n",
    "\n",
    "The **simplest thing one could do** is to ignore all the fancy maths and just take the **plain average of the branch logits**, or even pick *“whichever modality is most confident”* and run with it.  \n",
    "That baseline is fast and embarrassingly parallel, but it **throws away the chance for modalities to compensate for each other’s blind spots**—exactly what we need when spectra are noisy, photometry is sparse, or PSF stamps are half‑clouded.\n",
    "\n",
    "Multimodal learning therefore revolves around **how** and **when** to mix information from different sources.  \n",
    "We distinguish three canonical strategies (Baltrušaitis + Ahuja + Morency, 2019):\n",
    "\n",
    "| Strategy | Fusion equation | Pros | Cons |\n",
    "|----------|-----------------|------|------|\n",
    "| **Early fusion** | $ f_\\text{early}(x^{(1)},…,x^{(M)}) = g\\bigl([\\![ h_1(x^{(1)}) ; … ; h_M(x^{(M)}) ]\\!]\\bigr) $ | captures low‑level cross‑modal cues, single end‑to‑end model | modality mis‑alignment, huge input‑dim |\n",
    "| **Intermediate fusion** | $ f_\\text{inter}(…) = g\\bigl(h_1(x^{(1)}), …, h_M(x^{(M)})\\bigr) $ with cross‑attention / co‑transformers | flexible, selective interaction | heavier, harder to debug |\n",
    "| **Late fusion** | $ f_\\text{late} = \\sum_{m=1}^{M} w_m \\, f_m(x^{(m)}) $ | simple, debuggable, modular | may miss fine cross‑modal nuance |\n",
    "\n",
    "Mathematically, if $ f_m: \\mathcal{X}^{(m)} \\to \\mathbb{R}^C $ outputs logits for each class, then\n",
    "\n",
    "$$\n",
    "\\hat{y}= \\operatorname*{arg\\,max}_c \n",
    "\\Bigl[\\,\n",
    "\\sum_{m=1}^{M} w_m \\, \\underbrace{f_m\\bigl(x^{(m)}\\bigr)}_{\\text{branch logits}}\n",
    "\\Bigr]_c ,\n",
    "\\quad\n",
    "\\text{with } w_m \\ge 0,\\; \\sum_{m} w_m = 1.\n",
    "$$\n",
    "\n",
    "### Gated late‑fusion (what we implement)\n",
    "\n",
    "Instead of fixed weights, we **learn** a *gate*:\n",
    "\n",
    "1. Stack branch logits: $ \\ell = [f_1 ; f_2 ; … ; f_M] ∈ ℝ^{M·C} $  \n",
    "2. Pass through a small MLP:  \n",
    "   $ \\mathbf{w} = \\text{softmax}\\bigl(W_2 \\,\\sigma(W_1 \\ell)\\bigr) ∈ ℝ^{M} $  \n",
    "3. Final logits: $ \\tilde{f} = \\sum_{m} w_m f_m $\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* **Sample‑adaptive** – the model can down‑weight noisy spectra or missing images.  \n",
    "* **Modular** – you can hot‑swap any branch without retraining the others (fine‑tune gate).  \n",
    "* **Explainable** – inspect $ \\mathbf{w} $ to see which modality “won”.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "* Gate only sees **logits**, not deep embeddings → cannot exploit low‑level synergy.  \n",
    "* Requires **all modalities at inference time** (though you can zero‑pad missing ones).\n",
    "\n",
    "> **💡 Try This:** Benchmark at least **two** fusion baselines against gated fusion:  \n",
    "> * **Uniform‑average logits** – $ \\tilde{f} = \\frac{1}{M}\\sum_m f_m $.  \n",
    "> * **Uniform‑average encodings** – average branch embeddings *before* the classification head.  \n",
    "> * **Concatenate‑then‑MLP** – stack embeddings and learn one big predictor (classic early fusion).  \n",
    "> * **Confidence‑weighted average** – weight logits by their inverse entropy.  \n",
    "> * **Cross-Attention Fusion** –  \n",
    "> Report BALANCED ACCURACY and plot how fusion weights evolve over training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7072ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalClassifier(nn.Module):\n",
    "    \"\"\"Three branches → gated late‑fusion.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # phot + spectra\n",
    "        self.phot = PhotometryEncoder()\n",
    "        self.spec = SpectraEncoder()\n",
    "        fusion_dim = cfg.d_model + cfg.fc_units_spec\n",
    "        self.head  = nn.Sequential(nn.Linear(fusion_dim, 512),\n",
    "                                   nn.ReLU(), nn.Dropout(0.3),\n",
    "                                   nn.Linear(512, NUM_CLASSES))\n",
    "\n",
    "        # astroMiNN\n",
    "        self.astro = astroMiNN()\n",
    "\n",
    "        # gated fusion\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(NUM_CLASSES * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, phot, mask, spec, meta, img):\n",
    "        h_phot = self.phot(phot, mask)\n",
    "        h_spec = self.spec(spec)\n",
    "        logits_ps = self.head(torch.cat([h_phot, h_spec], 1))\n",
    "        logits_im = self.astro(meta, img)\n",
    "        stacked   = torch.cat([logits_ps, logits_im], 1)\n",
    "        w = torch.softmax(self.gate(stacked), 1)\n",
    "        final_logits = w[:, 0:1] * logits_ps + w[:, 1:2] * logits_im\n",
    "        return final_logits\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"γ‑weighted focal loss with optional α balancing.\"\"\"\n",
    "    def __init__(self, gamma=cfg.gamma, alpha=None):\n",
    "        super().__init__()\n",
    "        self.g, self.a = gamma, alpha\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        logp = F.log_softmax(logits, 1); p = logp.exp()\n",
    "        idx  = torch.arange(logits.size(0), device=logits.device)\n",
    "        logp_t, p_t = logp[idx, target], p[idx, target]\n",
    "        loss = -((1 - p_t) ** self.g) * logp_t\n",
    "        if self.a is not None:\n",
    "            loss *= self.a[target]\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81d42c",
   "metadata": {},
   "source": [
    "## 6 · Data Preparation\n",
    "\n",
    "Pipeline summary:\n",
    "\n",
    "1. **File sanity check:** drop rows whose `.npy` bundle is missing.  \n",
    "2. **Random 80 % / 20 % split** for train/val (*stratified* would be nicer—exercise!).  \n",
    "3. **Spectra StandardScaler** fitted on training set only.  \n",
    "4. **Photometry channel mean ± std** for z‑score normalisation.  \n",
    "5. **Class‑balanced α** for focal‑loss: \\( \\alpha_c ∝ 1/\\sqrt{N_c} \\).\n",
    "\n",
    "> **💡 Try This:** instead of `StandardScaler`, fit a **RobustScaler** to down‑weight spectral outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727936a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Load CSV and sanity‑check files\n",
    "exist_mask = full_df['file'].apply(lambda f: (Path(cfg.root_path) / f).exists())\n",
    "missing = (~exist_mask).sum()\n",
    "if missing:\n",
    "    warnings.warn(f\"dropping {missing} samples with missing .npy\")\n",
    "full_df = full_df[exist_mask].reset_index(drop=True)\n",
    "\n",
    "# 6.2 Train/val split --------------------------------------------------------\n",
    "train_df, val_df = np.split(full_df.sample(frac=1, random_state=cfg.seed),\n",
    "                            [int(0.8 * len(full_df))])\n",
    "\n",
    "# 6.3 Spectra scaler ---------------------------------------------------------\n",
    "spec_scaler = StandardScaler()\n",
    "for f in train_df['file']:\n",
    "    arr = np.nan_to_num(np.load(Path(cfg.root_path) / f, allow_pickle=True).item()['spectra'][0])\n",
    "    spec_scaler.partial_fit(arr.reshape(1, -1))\n",
    "\n",
    "# 6.4 Datasets & loaders -----------------------------------------------------\n",
    "train_ds = MultiModalDataset(train_df, cfg.root_path, spec_scaler=spec_scaler,\n",
    "                             grid_size=cfg.grid_size)\n",
    "phot_mean, phot_std = compute_phot_mean_std(train_ds)\n",
    "train_ds.phot_mean, train_ds.phot_std = phot_mean, phot_std\n",
    "\n",
    "val_ds = MultiModalDataset(val_df, cfg.root_path, phot_mean, phot_std,\n",
    "                           spec_scaler, cfg.grid_size)\n",
    "\n",
    "collate = build_collate(phot_mean, phot_std)\n",
    "train_ld = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                      num_workers=cfg.num_workers, collate_fn=collate, drop_last=True)\n",
    "val_ld   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                      num_workers=cfg.num_workers, collate_fn=collate)\n",
    "\n",
    "# 6.5 α balancing for focal loss --------------------------------------------\n",
    "labels_train = torch.tensor([train_ds[i][-1] for i in range(len(train_ds))])\n",
    "counts = torch.bincount(labels_train, minlength=NUM_CLASSES).float()\n",
    "alpha  = (1.0 / torch.sqrt(counts)).to(device)\n",
    "\n",
    "print('✔️  Data ready. Train batches:', len(train_ld), '| Val batches:', len(val_ld))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354a265",
   "metadata": {},
   "source": [
    "## 7 · Training Loop\n",
    "\n",
    "We use:\n",
    "\n",
    "* **AdamW** optimiser  \n",
    "* **Cosine‑Annealing‑Warm‑Restarts** LR schedule  \n",
    "* **Focal loss** with γ = 2 to fight class imbalance  \n",
    "* **Gradient clipping** at 1.0 norm  \n",
    "* **Early stopping** on *macro AUPRC* with patience = 50 epochs\n",
    "\n",
    "Keep an eye on **Weights & Biases** dashboards: learning curves, ROC, PR, and gate weights per batch.\n",
    "\n",
    "> **💡 Try This:** try early stopping on other criteria, such as Balanced Accuracy or Validation Loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=cfg.project, mode=cfg.wandb_mode, config=vars(cfg))\n",
    "\n",
    "model = MultiModalClassifier().to(device)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "crit  = FocalLoss(cfg.gamma, alpha)\n",
    "\n",
    "best_auprc, no_imp = 0.0, 0\n",
    "ckpt_path = Path(wandb.run.dir) / 'best.pt'\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    tl, correct, N = 0., 0, 0\n",
    "    all_prob, all_lab = [], []\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for phot, mask, spec, meta, img, y in loader:\n",
    "            phot, mask, spec = phot.to(device), mask.to(device), spec.to(device)\n",
    "            meta, img, y     = meta.to(device), img.to(device), y.to(device)\n",
    "\n",
    "            logits = model(phot, mask, spec, meta, img)\n",
    "            loss   = crit(logits, y)\n",
    "\n",
    "            if train:\n",
    "                opt.zero_grad(); loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                opt.step(); sched.step()\n",
    "\n",
    "            tl += loss.item() * y.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "            N += y.size(0)\n",
    "            all_prob.append(F.softmax(logits, 1).detach().cpu())\n",
    "            all_lab.append(y.cpu())\n",
    "\n",
    "    probs = torch.cat(all_prob); labs = torch.cat(all_lab)\n",
    "    acc   = correct / N\n",
    "    auroc = roc_auc_score(labs, probs, multi_class='ovr', average='macro')\n",
    "    return tl / N, acc, auroc, labs.numpy(), probs.numpy()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "for ep in range(1, cfg.epochs + 1):\n",
    "    tr_loss, tr_acc, tr_auc, *_ = run_epoch(train_ld, True)\n",
    "    va_loss, va_acc, va_auc, yt, yp = run_epoch(val_ld, False)\n",
    "\n",
    "    y_onehot = label_binarize(yt, classes=np.arange(NUM_CLASSES))\n",
    "    va_auprc = average_precision_score(y_onehot, yp, average='macro')\n",
    "\n",
    "    wandb.log({'epoch': ep,\n",
    "               'train/loss': tr_loss, 'train/acc': tr_acc,\n",
    "               'val/loss': va_loss, 'val/acc': va_acc,\n",
    "               'val/auroc_macro': va_auc, 'val/auprc_macro': va_auprc,\n",
    "               'lr': sched.get_last_lr()[0]})\n",
    "\n",
    "    if va_auprc > best_auprc:\n",
    "        best_auprc, no_imp = va_auprc, 0\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        wandb.save(str(ckpt_path))\n",
    "    else:\n",
    "        no_imp += 1\n",
    "        if no_imp >= cfg.patience:\n",
    "            print('⏹ Early stopping'); break\n",
    "    print(f'[ep {ep:03d}] tr_acc={tr_acc:.3f}  va_acc={va_acc:.3f}  va_AUPRC={va_auprc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c8717",
   "metadata": {},
   "source": [
    "## 8 · Evaluation & Visualisation\n",
    "\n",
    "For each split we log:\n",
    "\n",
    "* *Metrics*: Acc, F1‑macro, AUROC‑macro, AUPRC‑macro, Top‑2/3 accuracy  \n",
    "* *Plots*: ROC curve per class, PR curve, confusion matrix, confidence histograms  \n",
    "\n",
    "Interpretation tips:\n",
    "\n",
    "* **ROC vs PR:** PR is more sensitive to class imbalance—optimise AUPRC here.  \n",
    "* **Confidence histogram:** a well‑calibrated classifier shows sharp peaks near 0 and 1 for each class.  \n",
    "* **Gate weights:** if $ w_\\text{phot} $ is systematically low, maybe the photometry encoder is under‑trained. That is very common.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a08698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, split):\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    loss, acc, auc_macro, y, p = run_epoch(loader, False)\n",
    "    y_pred = p.argmax(1)\n",
    "    # Your code here...\n",
    "\n",
    "val_metrics = evaluate(val_ld, 'val')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278241ba-031e-41d0-8134-2e50df89d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Test set ---------------------------------------------------------------\n",
    "\n",
    "## We will do this later!\n",
    "\n",
    "\n",
    "#test_df = pd.read_csv(cfg.test_csv_path)\n",
    "#test_df['type'] = test_df['type'].map(ORIG2BROAD)\n",
    "#test_ds = MultiModalDataset(test_df, cfg.test_root, phot_mean, phot_std,\n",
    "                            spec_scaler, cfg.grid_size)\n",
    "#test_ld = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                     #num_workers=cfg.num_workers, collate_fn=collate)\n",
    "#test_metrics = evaluate(test_ld, 'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4b108",
   "metadata": {},
   "source": [
    "## 9 · Next Steps & Stretch Goals \n",
    "\n",
    "| Goal | Suggestion |\n",
    "|------|------------|\n",
    "| **Better Encoders** | Try different encoders for all modalities. Some times, less can be more! |\n",
    "| **Better Fusion** | No wrong answers here. Be creative! Talking about that... |\n",
    "| **Early‑fusion Transformer** | Concatenate photometry fluxes, binned spectra, and PSF embeddings into one mega‑sequence; add modality tokens. Take a look at *Perceiver I/O*! |\n",
    "| **Full taxonomy** | Expand `ORIG2BROAD` mapping to all subclasses; swap loss to class‑balanced focal. |\n",
    "| **Early‑fusion Transformer** | Concatenate photometry fluxes, binned spectra, and PSF embeddings into one mega‑sequence; add modality tokens. |\n",
    "| **Co‑attention (Inter‑fusion)** | Implement *LXMERT*‑style cross‑attention between phot and spec encoders. |\n",
    "| **Self‑supervised pre‑training** | Masked‑value reconstruction on light‑curves; BYOL on PSF stamps. |\n",
    "| **Uncertainty‑aware gating** | Use logits’ variance or entropy to re‑weight unreliable modalities. |\n",
    "| **Hyper‑param sweep** | `wandb.sweep` on LR, γ, #heads, experts. |\n",
    "| **Model ensembling** | Logit‑average 5 checkpoints with different seeds. |\n",
    "\n",
    "\n",
    "**Leaderboard metric:** *BALANCED ACCURACY on the hidden test‑set.*  \n",
    "Push your best run to Slack Day05, with some beautiful plots :)\n",
    "\n",
    "Happy hacking!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc91668-50bd-4a9a-b309-39dc6a8f17f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nmma_env)",
   "language": "python",
   "name": "nmma_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
