{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f5bfef",
   "metadata": {},
   "source": [
    "# Multimodal Earlyâ€‘Classification of ZTF Transients  \n",
    "### *A mini â€œAppleCiDErâ€ hackathon*\n",
    "\n",
    "\n",
    "### Get the data here! https://drive.google.com/drive/u/0/folders/19EUnPU4AyN8Y0dF_CYsNHHxz94qjL1q9 \n",
    "\n",
    "You should download all the data_train_multi files.\n",
    "\n",
    "\n",
    "\n",
    "Welcome to the **grandâ€‘finale hackathon** of the 2025 ZTF Summer School!  \n",
    "In the next few hours you will build, train, and evaluate a **multimodal classifier** that decidesâ€”within the **firstâ€¯10 days** of discoveryâ€”what kind of optical transient we are watching.\n",
    "\n",
    "Why does *early* classification matter?\n",
    "\n",
    "* *Rapid followâ€‘up:* scarce resources (Spectrographs, JWST, HST, VLAâ€¦) must be allocated **before** a transient fades.  \n",
    "* *Fast transients physics:* the more data we have, the better we can constrain models.  \n",
    "* *Event brokers:* surveys such as ZTF, Rubin, ... stream millions of alerts/night; automated early triage is critical!\n",
    "\n",
    "We will focus our taxonomy into **five broad classes** to keep training fast for the time scale of the hackathon:\n",
    "\n",
    "| Broad class | Typical zoo members |\n",
    "|-------------|--------------------|\n",
    "| **SNI**  | SNâ€¯Ia, Ib, Ic |\n",
    "| **SNII** | SNâ€¯II, IIâ€‘P, IIn, IIâ€‘b |\n",
    "| **CV** | Cataclysmic Variables |\n",
    "| **AGN** | Active Galactic Nuclei |\n",
    "| **TDE** |  Tidal Disruption Events |\n",
    "\n",
    "In spirit we are reproducing a toy version of  \n",
    "> **â€œApplying Multimodal Learning to Classify Transient Detections Early (AppleCiDEr) I: Dataset, Methods, and Infrastructureâ€** â€“ *AJ Brown et al., 2025*.\n",
    "\n",
    "Good luck and have fun!\n",
    "\n",
    "As a starting point, we are providing an early development version of AppleCider. The cells are supposed to be more of a guide then a fully functional code; There will be places where you will either have to complete available code or write your own. Feel free to transfer the parts relevant to you to your own coding environment.\n",
    "\n",
    "This version of the **multimodal classifier** combines:\n",
    "\n",
    "| Modality | Encoder | Key Idea |\n",
    "|----------|---------|----------|\n",
    "| **Photometry** | Transformer | temporal selfâ€‘attention on lightâ€‘curves |\n",
    "| **Spectra** | CNN | harvesting power from 2D CNNs |\n",
    "| **ImageÂ +Â Metadata** | astroMiNN | Mixtureâ€‘ofâ€‘Experts fusion tower |\n",
    "| **Fusion** | Gated lateâ€‘fusion | sampleâ€‘adaptive weighting of branch logits |\n",
    "  \n",
    "Everything is thoroughly annotated, but if you have any questions about any of the bits, please let us know :) \n",
    "\n",
    "> **Your challenge:** improve BALANCED ACCURACY of the multi-modal classifier. Feel free to exercise all of your creativity and substitute any of the blocks in this code, to implement any technique you like. We left some tips that might be worth exploring on the â€œðŸ’¡Â TryÂ Thisâ€ cells sprinkled throughout.\n",
    ">\n",
    "> Happy Hacking!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5a89b",
   "metadata": {},
   "source": [
    "## 0Â Â·Â Setup & Dependencies\n",
    "\n",
    "- We suggest making a fresh conda environment for this challenge.  \n",
    "- If you run locally, uncomment the cell below to install some of the important Python packages.\n",
    "\n",
    "Feel free to add any extra libraries you need (like `tqdm`, `rich`, `torchmetrics`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72029ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install numpy pandas scikit-learn seaborn wandb tqdm rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f06909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('service')` is a no-op as it is now the default behavior.\n"
     ]
    }
   ],
   "source": [
    "import os, math, argparse, warnings, json, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, f1_score,\n",
    "                             precision_score, recall_score, average_precision_score,\n",
    "                             confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
    "                             top_k_accuracy_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "# PrettyÂ plots\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "plt.rcParams['figure.autolayout'] = True\n",
    "wandb.require(\"service\")\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0061528",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit the **data paths** below if you stashed the CSVs or `.npy` files in a different directory.  \n",
    "All other hyperâ€‘parameters are sane defaults, but you are encouraged to play:\n",
    "\n",
    "* Transformer depth, heads, dropout  \n",
    "* CNN kernel sizes / channels  \n",
    "* Number of experts in astroMiNN  \n",
    "* Learning rate schedule, batch size, $\\gamma$ for focalâ€‘loss\n",
    "\n",
    "> **ðŸ’¡ Try This:** crank `d_model` to 256 and halve `epochs` while using cosine restarts; Sometimes, bigger models converge faster, but can be less stable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b09c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    # â”€â”€ data paths (EDIT ME) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    csv_path      = \"data_train_multi_10.csv\"\n",
    "    root_path     = \"./data_train_multi/day10\"\n",
    "    test_csv_path = \"data_test_multi_10.csv\"\n",
    "    test_root     = \"./data_test_multi/day10\"\n",
    "\n",
    "    # â”€â”€ experiment tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    project       = \"multimodal_late_fusion_gated\"\n",
    "    wandb_mode    = \"disabled\"         # 'disabled' for offline\n",
    "\n",
    "    # â”€â”€ optimisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    batch_size    = 128\n",
    "    epochs        = 150\n",
    "    lr            = 1e-6\n",
    "    weight_decay  = 1e-2\n",
    "    patience      = 50\n",
    "    num_workers   = 10\n",
    "    gamma         = 2.0              # focalâ€‘loss Î³\n",
    "    seed          = 42\n",
    "\n",
    "    # â”€â”€ photometry Transformer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    d_model       = 128\n",
    "    num_layers    = 8\n",
    "    num_heads     = 4\n",
    "    dropout       = 0.3\n",
    "    max_len       = 500\n",
    "\n",
    "    # â”€â”€ spectra CNN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    grid_size           = 59\n",
    "    conv1_filters       = 32\n",
    "    conv1_kernel        = 7\n",
    "    conv2_filters       = 64\n",
    "    conv2_kernel        = 7\n",
    "    pool_size           = 2\n",
    "    fc_units_spec       = 2048\n",
    "    fc_dropout          = 0.3\n",
    "\n",
    "    # â”€â”€ astroMiNN (image + metadata) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    astro_input_dim           = 9\n",
    "    astro_hidden_dim          = 128\n",
    "    astro_proj_dim            = 256\n",
    "    astro_num_experts         = 10\n",
    "    astro_num_fusion_experts  = 5\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# Determinism helpers\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "random.seed(cfg.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b4b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ taxonomy ---------------------------------------------------------------\n",
    "BROAD_CLASSES = [\"SNI\", \"SNII\", \"Cataclysmic\", \"AGN\", \"TDE\"]\n",
    "NUM_CLASSES   = len(BROAD_CLASSES)\n",
    "ORIG2BROAD = {\n",
    "    \"SN Ia\":\"SNI\",\"SN Ib\":\"SNI\",\"SN Ic\":\"SNI\",\n",
    "    \"SN II\":\"SNII\",\"SN IIP\":\"SNII\",\"SN IIn\":\"SNII\",\"SN IIb\":\"SNII\",\n",
    "    \"Cataclysmic\":\"Cataclysmic\",\"AGN\":\"AGN\",\"Tidal Disruption Event\":\"TDE\"\n",
    "}\n",
    "BROAD2ID = {c:i for i,c in enumerate(BROAD_CLASSES)}\n",
    "\n",
    "# â”€â”€ preâ€‘computed metadata normalisation stats ------------------------------\n",
    "METADATA_STATS = {\n",
    "    'sgscore1': {'mean': 0.236, 'std': 0.266},\n",
    "    'sgscore2': {'mean': 0.401, 'std': 0.328},\n",
    "    'distpsnr1': {'mean': 3.151, 'std': 3.757},\n",
    "    'distpsnr2': {'mean': 9.269, 'std': 6.323},\n",
    "    'nmtchps': {'mean': 9.231, 'std': 8.089},\n",
    "    'sharpnr': {'mean': 0.253, 'std': 0.256},\n",
    "    'scorr': {'mean': 22.089, 'std': 16.757},\n",
    "    'ra': {'mean': 0, 'std': 1},\n",
    "    'dec': {'mean': 0, 'std': 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc3503",
   "metadata": {},
   "source": [
    "## 2Â Â·Â DataÂ Exploration\n",
    "\n",
    "\n",
    "Before touching any code, letâ€™s *look* at the dataset. Your first exercise is to obtain as much information from the dataset as you can by plotting data for individual objects, statistics of relevant features...\n",
    "\n",
    "* **Rows:** individual ZTF objects (one per transient)  \n",
    "* **file:** `.npy` bundle containing the four modalities  \n",
    "* **type:** label map \n",
    "\n",
    "As a first suggestion, letâ€™s peek at the CSV that indexes our multimodal samples and visualise the class distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfbb4943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>file</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZTF17aaaemzh</td>\n",
       "      <td>ZTF17aaaemzh_alert_0.npy</td>\n",
       "      <td>Cataclysmic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZTF17aaaikab</td>\n",
       "      <td>ZTF17aaaikab_alert_9.npy</td>\n",
       "      <td>Cataclysmic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZTF17aaajazz</td>\n",
       "      <td>ZTF17aaajazz_alert_1.npy</td>\n",
       "      <td>AGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZTF17aaajlbs</td>\n",
       "      <td>ZTF17aaajlbs_alert_10.npy</td>\n",
       "      <td>Cataclysmic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZTF17aaajlfw</td>\n",
       "      <td>ZTF17aaajlfw_alert_13.npy</td>\n",
       "      <td>Cataclysmic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name                       file         type\n",
       "0  ZTF17aaaemzh   ZTF17aaaemzh_alert_0.npy  Cataclysmic\n",
       "1  ZTF17aaaikab   ZTF17aaaikab_alert_9.npy  Cataclysmic\n",
       "2  ZTF17aaajazz   ZTF17aaajazz_alert_1.npy          AGN\n",
       "3  ZTF17aaajlbs  ZTF17aaajlbs_alert_10.npy  Cataclysmic\n",
       "4  ZTF17aaajlfw  ZTF17aaajlfw_alert_13.npy  Cataclysmic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 8,679\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_df = pd.read_csv(cfg.csv_path)\n",
    "full_df['type'] = full_df['type'].map(ORIG2BROAD)\n",
    "display(full_df.head())\n",
    "\n",
    "print(f\"Total samples: {len(full_df):,}\")\n",
    "class_counts = full_df['type'].value_counts().reindex(BROAD_CLASSES).fillna(0)\n",
    "class_counts.plot(kind='bar', rot=0)\n",
    "plt.title('Class Distribution'); plt.ylabel('# objects'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a8ac3-76b4-4456-a98d-d683f1c03810",
   "metadata": {},
   "source": [
    "> **ðŸ’¡ TryÂ This:** Ask yourself whether this level of class imbalance will significantly affect your training. If yes, what techniques could you employ to fight it? Oversampling the minority classes or Undersampling the majority classes are two common techniques, but there are many others out there!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00396db2",
   "metadata": {},
   "source": [
    "## 3 Â· DatasetÂ &Â Dataloaders\n",
    "\n",
    "### Anatomy of one sample\n",
    "\n",
    "| Tensor | Shape | Meaning |\n",
    "|--------|-------|---------|\n",
    "| `phot` | `(T,â€¯4)` | ZTF lightâ€‘curve (g/r flux, fluxâ€‘err, mjd) |\n",
    "| `spec_img` | `(1,â€¯59,â€¯59)` | Raw spectrum â–¸ zâ€‘score â–¸ square grid |\n",
    "| `meta_vec` | `(9,)` | Contextual features (sgscore, distpsnr, â€¦) |\n",
    "| `image` | `(3,â€¯H,â€¯W)` | Triplet of Image Cutouts, scaled toÂ [0,â€¯1] |\n",
    "| `label` | scalar | 0â€“4 for the 5 broad classes |\n",
    "\n",
    "Key implementation tricks:\n",
    "\n",
    "1. PaddingÂ +Â masking for variableâ€‘length photometry sequences so the Transformer ignores padded tokens.  \n",
    "2. StandardScaler on spectra voxelsâ€”fitted on *training* set only (very important, to avoid leakage!).  \n",
    "3. Onâ€‘theâ€‘fly squareâ€‘grid mapping so you can easily swapâ€‘in different `grid_size` or even 1â€‘D CNNs.\n",
    "\n",
    "> **ðŸ’¡Â TryÂ This:** If preprocessing the spectra is taking too much performance out of your dataloader, cache the mapped spectra on disk for a decent speedâ€‘up at the cost of some RAM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2a82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset(Dataset):\n",
    "    \"\"\"Custom dataset returning five tensors per sample.\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, root: Path,\n",
    "                 phot_mean=None, phot_std=None, spec_scaler=None,\n",
    "                 grid_size=cfg.grid_size, transform=None):\n",
    "        self.df   = df.reset_index(drop=True)\n",
    "        self.root = Path(root)\n",
    "        self.phot_mean = phot_mean\n",
    "        self.phot_std  = phot_std\n",
    "        self.spec_scaler = spec_scaler\n",
    "        self.grid_size   = grid_size\n",
    "        self.transform   = transform\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def _load_sample(self, row):\n",
    "        sample = np.load(self.root / row['file'], allow_pickle=True).item()\n",
    "        phot   = torch.tensor(sample['photometry'], dtype=torch.float32)\n",
    "        spec   = np.nan_to_num(sample['spectra'][0])\n",
    "\n",
    "        # --- metadata vector (zâ€‘score or minâ€‘max) ----------\n",
    "        m = sample['metadata']\n",
    "        meta_vec = torch.tensor([\n",
    "            (m['sgscore1']  - METADATA_STATS['sgscore1']['mean']) / METADATA_STATS['sgscore1']['std'],\n",
    "            (m['sgscore2']  - METADATA_STATS['sgscore2']['mean']) / METADATA_STATS['sgscore2']['std'],\n",
    "            (m['distpsnr1'] - METADATA_STATS['distpsnr1']['mean']) / METADATA_STATS['distpsnr1']['std'],\n",
    "            (m['distpsnr2'] - METADATA_STATS['distpsnr2']['mean']) / METADATA_STATS['distpsnr2']['std'],\n",
    "            (m['nmtchps']   - METADATA_STATS['nmtchps']['mean'])  / METADATA_STATS['nmtchps']['std'],\n",
    "            (m['sharpnr']   - METADATA_STATS['sharpnr']['mean'])  / METADATA_STATS['sharpnr']['std'],\n",
    "            (m['scorr']     - METADATA_STATS['scorr']['mean'])    / METADATA_STATS['scorr']['std'],\n",
    "            (m['ra'] / 360.0),\n",
    "            ((m['dec'] + 90.0) / 180.0)\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        # --- PSF stamp -------------------------------------\n",
    "        img = torch.tensor(sample['images'], dtype=torch.float32).permute(2,0,1) / 255.0\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return phot, spec, meta_vec, img\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phot, spec, meta_vec, img = self._load_sample(row)\n",
    "\n",
    "        # spectra â†’ square grid\n",
    "        spec_std = self.spec_scaler.transform([spec])[0]\n",
    "        needed   = self.grid_size ** 2\n",
    "        if spec_std.shape[0] < needed:\n",
    "            spec_std = np.pad(spec_std, (0, needed - spec_std.shape[0]))\n",
    "        elif spec_std.shape[0] > needed:\n",
    "            spec_std = spec_std[:needed]\n",
    "        spec_img = torch.tensor(spec_std.reshape(self.grid_size, self.grid_size),\n",
    "                                dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label = BROAD2ID[row['type']]\n",
    "        return phot, spec_img, meta_vec, img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20fd251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phot_mean_std(ds):\n",
    "    \"\"\"Perâ€‘channel mean/std over an *unâ€‘padded* photometry dataset.\"\"\"\n",
    "    sums, sqs, n = torch.zeros(4), torch.zeros(4), 0\n",
    "    for phot, *_ in ds:\n",
    "        sums += phot.sum(0)\n",
    "        sqs  += (phot ** 2).sum(0)\n",
    "        n    += phot.shape[0]\n",
    "    mean = (sums / n)[None, None, :]\n",
    "    std  = torch.sqrt(sqs / n - mean.squeeze() ** 2)[None, None, :]\n",
    "    return mean, std\n",
    "\n",
    "def build_collate(mean, std):\n",
    "    def collate(batch):\n",
    "        phot_seqs, spec_imgs, meta_vecs, img_tensors, labels = zip(*batch)\n",
    "\n",
    "        spec_batch = torch.stack(spec_imgs)\n",
    "        meta_batch = torch.stack(meta_vecs)\n",
    "        img_batch  = torch.stack(img_tensors)\n",
    "\n",
    "        lengths = [p.shape[0] for p in phot_seqs]\n",
    "        padded  = pad_sequence(phot_seqs, batch_first=True)\n",
    "        mask    = torch.stack([\n",
    "            torch.cat([torch.zeros(l), torch.ones(padded.shape[1]-l)])\n",
    "            for l in lengths\n",
    "        ]).bool()\n",
    "        normed  = (padded - mean) / (std + 1e-8)\n",
    "        return normed, mask, spec_batch, meta_batch, img_batch, torch.tensor(labels)\n",
    "    return collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afacbe5",
   "metadata": {},
   "source": [
    "## 4 Â· ModalÂ Encoders\n",
    "\n",
    "### PhotometryÂ Transformer Â \n",
    "Temporal selfâ€‘attention excels at capturing irregular cadence and longâ€‘range dependencies.  \n",
    "We prepend a learnable **[CLS] token** whose output embedding summarises the entire lightâ€‘curve.\n",
    "\n",
    "> **ðŸ’¡Â TryÂ This:** Can just the [CLS] token hold all the information about the transient classes or would be better to use all output encondings from the Transformer model? \n",
    "\n",
    "\n",
    "> **ðŸ’¡Â TryÂ This:** Can we include a better Positional Encoding, that doesn't rely on the index of the photometric point, but actually encodes the timing between observations? Learnable, time-aware positional encoding was shown to do better in time-series. \n",
    "\n",
    "*Input*: padded sequence `x = (time, g_flux, r_flux, i_flux) âˆˆ â„^{BÃ—TÃ—4}`.   \n",
    "*PosEnc*: $ \\text{PE}(t,2i)=\\sin(t/10000^{2i/d}) $, $ \\text{PE}(t,2i+1)=\\cos(t/10000^{2i/d}) $\n",
    "\n",
    "### SpectraÂ CNN Â \n",
    "We reshape each 1â€‘D spectrum into a 2â€‘D postageâ€‘stamp and apply two convolutional blocks. Cool reference about Spectrum Classification: *DASH* (MuthukrishnaÂ etÂ al.Â 2019).  \n",
    "Feel free to:\n",
    "\n",
    "* switch to a **1â€‘D residual CNN**  \n",
    "* treat the spectrum as a **sentence** and use a Transformer  \n",
    "* replace pooling with **strided convolutions**\n",
    "\n",
    "### astroMiNN (ImageÂ +Â Metadata) Â \n",
    "A lightâ€‘weight Mixtureâ€‘ofâ€‘Experts inspired by *Mixtureâ€‘ofâ€‘Experts for Multimodal Supernova Classification* (CaupinÂ etÂ al.,Â 2024).  \n",
    "Separate â€œtowersâ€ encode PSFâ€‘shape, localisation, and contextual features, then a **fusionâ€‘router** softly combines them.\n",
    "\n",
    "> **ðŸ’¡Â TryÂ This:** swap the Images tower for a pretrained ViT â€œpatchifiedâ€ stamp!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc683bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(1e4) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):  # (B,Â T,Â D)\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class PhotometryEncoder(nn.Module):\n",
    "    \"\"\"A CLSâ€‘token Transformer encoder for irregular lightâ€‘curves.\"\"\"\n",
    "    def __init__(self, input_dim=4):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(input_dim, cfg.d_model)\n",
    "        self.cls   = nn.Parameter(torch.zeros(1, 1, cfg.d_model))\n",
    "        self.pos   = PositionalEncoding(cfg.d_model, cfg.max_len + 1)\n",
    "        enc_layer  = nn.TransformerEncoderLayer(cfg.d_model, cfg.num_heads,\n",
    "                                                cfg.d_model * 4, cfg.dropout, batch_first=True)\n",
    "        self.enc   = nn.TransformerEncoder(enc_layer, cfg.num_layers)\n",
    "        self.norm  = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        B = x.size(0)\n",
    "        tok = self.cls.expand(B, -1, -1)\n",
    "        x   = torch.cat([tok, self.embed(x)], 1)\n",
    "        pad = torch.cat([torch.zeros(B, 1, dtype=torch.bool, device=x.device), pad_mask], 1)\n",
    "        h   = self.enc(self.pos(x), src_key_padding_mask=pad)\n",
    "        return self.norm(h[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d44bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectraEncoder(nn.Module):\n",
    "    \"\"\"DASHâ€‘style 2â€‘layer CNN over a (1,Â 59,Â 59) spectra image.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, cfg.conv1_filters, cfg.conv1_kernel,\n",
    "                               padding=cfg.conv1_kernel // 2)\n",
    "        self.bn1   = nn.BatchNorm2d(cfg.conv1_filters)\n",
    "        self.pool1 = nn.MaxPool2d(cfg.pool_size)\n",
    "        self.conv2 = nn.Conv2d(cfg.conv1_filters, cfg.conv2_filters, cfg.conv2_kernel,\n",
    "                               padding=cfg.conv2_kernel // 2)\n",
    "        self.bn2   = nn.BatchNorm2d(cfg.conv2_filters)\n",
    "        self.pool2 = nn.MaxPool2d(cfg.pool_size)\n",
    "        out_size   = cfg.grid_size // (cfg.pool_size * cfg.pool_size)\n",
    "        self.flat  = nn.Flatten()\n",
    "        self.fc1   = nn.Linear(cfg.conv2_filters * out_size * out_size, cfg.fc_units_spec)\n",
    "        self.drop  = nn.Dropout(cfg.fc_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x))); x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x))); x = self.pool2(x)\n",
    "        x = self.flat(x)\n",
    "        return self.drop(F.relu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f495659",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CNN'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenseNet\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# NOTE: CoordCNNJointTower should be in the repo.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCNN\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoordCNNJointTower  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFeatureInteraction\u001b[39;00m(nn.Module):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'CNN'"
     ]
    }
   ],
   "source": [
    "from torchvision.ops import SqueezeExcitation\n",
    "from torchvision.models import DenseNet\n",
    "# NOTE: CoordCNNJointTower should be in the repo.\n",
    "from CNN import CoordCNNJointTower  # noqa\n",
    "\n",
    "class FeatureInteraction(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim, dim * 2)\n",
    "        self.gate = nn.Sequential(nn.Linear(dim * 2, dim), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.gate(self.proj(x))\n",
    "\n",
    "class astroMiNN(nn.Module):\n",
    "    \"\"\"ImageÂ +Â metadata tower with MoE fusion and classification.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        H, P = cfg.astro_hidden_dim, cfg.astro_proj_dim\n",
    "\n",
    "        # --- perâ€‘feature towers -------------------------------------------\n",
    "        self.psf_tower = nn.Sequential(\n",
    "            nn.Linear(2, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.spatial_tower = nn.Sequential(\n",
    "            nn.Linear(3, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.nst1_tower = nn.Sequential(\n",
    "            nn.Linear(2, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.nst2_tower = nn.Sequential(\n",
    "            nn.Linear(2, H), FeatureInteraction(H), nn.GELU(),\n",
    "            nn.LayerNorm(H), nn.Linear(H, P), nn.SiLU())\n",
    "\n",
    "        self.coord_cnn_tower = CoordCNNJointTower()  # imageâ†’P\n",
    "\n",
    "        # --- fusion Mixtureâ€‘ofâ€‘Experts ------------------------------------\n",
    "        self.fusion_experts = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(P * 5, 512), nn.LayerNorm(512),\n",
    "                          nn.GELU(), nn.Linear(512, P))\n",
    "            for _ in range(cfg.astro_num_fusion_experts)\n",
    "        ])\n",
    "        self.fusion_router = nn.Linear(P * 5, cfg.astro_num_fusion_experts)\n",
    "        self.proj_fuse     = nn.Sequential(nn.Linear(P, P), nn.LayerNorm(P), nn.GELU())\n",
    "\n",
    "        # --- classification Mixtureâ€‘ofâ€‘Experts ----------------------------\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(P, P // 2), nn.LayerNorm(P // 2),\n",
    "                          nn.GELU(), nn.Dropout(.1),\n",
    "                          nn.Linear(P // 2, NUM_CLASSES))\n",
    "            for _ in range(cfg.astro_num_experts)\n",
    "        ])\n",
    "        self.router = nn.Linear(P, cfg.astro_num_experts)\n",
    "\n",
    "    def forward(self, x_meta, image):\n",
    "        psf_feats = self.psf_tower(x_meta[:, [5, 6]])           # sharpnr, scorr\n",
    "        spatial   = self.spatial_tower(x_meta[:, [2, 3, 4]])    # distpsnr 1/2 + nmtchps\n",
    "        nsta      = self.nst1_tower(x_meta[:, [0, 2]])          # sgscore1 + distpsnr1\n",
    "        nstb      = self.nst2_tower(x_meta[:, [1, 3]])          # sgscore2 + distpsnr2\n",
    "        joint     = self.coord_cnn_tower(x_meta[:, [7, 8]], image)\n",
    "\n",
    "        all_feats = torch.cat([nsta, nstb, spatial, psf_feats, joint], 1)\n",
    "        fusion_w  = F.softmax(self.fusion_router(all_feats), 1)\n",
    "        expert_out = torch.stack([e(all_feats) for e in self.fusion_experts], 1)\n",
    "        fused     = torch.einsum('be,bej->bj', fusion_w, expert_out)\n",
    "        fused     = self.proj_fuse(fused)\n",
    "\n",
    "        router_logits = self.router(fused)\n",
    "        expert_w      = F.softmax(router_logits, 1)\n",
    "        expert_logits = torch.stack([e(fused) for e in self.experts], 2)\n",
    "        return torch.einsum('be,bce->bc', expert_w, expert_logits)  # logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23eba32",
   "metadata": {},
   "source": [
    "## 5 Â· ModalÂ Fusion \n",
    "\n",
    "The **simplest thing one could do** is to ignore all the fancy maths and just take the **plain average of the branch logits**, or even pick *â€œwhichever modality is most confidentâ€* and run with it.  \n",
    "That baseline is fast and embarrassingly parallel, but it **throws away the chance for modalities to compensate for each otherâ€™s blind spots**â€”exactly what we need when spectra are noisy, photometry is sparse, or PSF stamps are halfâ€‘clouded.\n",
    "\n",
    "Multimodal learning therefore revolves around **how** and **when** to mix information from different sources.  \n",
    "We distinguish three canonical strategies (BaltruÅ¡aitisâ€¯+â€¯Ahujaâ€¯+â€¯Morency,Â 2019):\n",
    "\n",
    "| Strategy | Fusion equation | Pros | Cons |\n",
    "|----------|-----------------|------|------|\n",
    "| **Early fusion** | $ f_\\text{early}(x^{(1)},â€¦,x^{(M)}) = g\\bigl([\\![ h_1(x^{(1)}) ; â€¦ ; h_M(x^{(M)}) ]\\!]\\bigr) $ | captures lowâ€‘level crossâ€‘modal cues, single endâ€‘toâ€‘end model | modality misâ€‘alignment, huge inputâ€‘dim |\n",
    "| **Intermediate fusion** | $ f_\\text{inter}(â€¦) = g\\bigl(h_1(x^{(1)}), â€¦, h_M(x^{(M)})\\bigr) $ with crossâ€‘attention / coâ€‘transformers | flexible, selective interaction | heavier, harder to debug |\n",
    "| **Late fusion** | $ f_\\text{late} = \\sum_{m=1}^{M} w_m \\, f_m(x^{(m)}) $ | simple, debuggable, modular | may miss fine crossâ€‘modal nuance |\n",
    "\n",
    "Mathematically, if $ f_m: \\mathcal{X}^{(m)} \\to \\mathbb{R}^C $ outputs logits for each class, then\n",
    "\n",
    "$$\n",
    "\\hat{y}= \\operatorname*{arg\\,max}_c \n",
    "\\Bigl[\\,\n",
    "\\sum_{m=1}^{M} w_m \\, \\underbrace{f_m\\bigl(x^{(m)}\\bigr)}_{\\text{branchÂ logits}}\n",
    "\\Bigr]_c ,\n",
    "\\quad\n",
    "\\text{with } w_m \\ge 0,\\; \\sum_{m} w_m = 1.\n",
    "$$\n",
    "\n",
    "### Gated lateâ€‘fusion (what we implement)\n",
    "\n",
    "Instead of fixed weights, we **learn** a *gate*:\n",
    "\n",
    "1. Stack branch logits: $ \\ell = [f_1 ; f_2 ; â€¦ ; f_M] âˆˆ â„^{MÂ·C} $  \n",
    "2. Pass through a small MLP:  \n",
    "   $ \\mathbf{w} = \\text{softmax}\\bigl(W_2 \\,\\sigma(W_1 \\ell)\\bigr) âˆˆ â„^{M} $  \n",
    "3. Final logits: $ \\tilde{f} = \\sum_{m} w_m f_m $\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* **Sampleâ€‘adaptive** â€“ the model can downâ€‘weight noisy spectra or missing images.  \n",
    "* **Modular** â€“ you can hotâ€‘swap any branch without retraining the others (fineâ€‘tune gate).  \n",
    "* **Explainable** â€“ inspect $ \\mathbf{w} $ to see which modality â€œwonâ€.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "* Gate only sees **logits**, not deep embeddings â†’ cannot exploit lowâ€‘level synergy.  \n",
    "* Requires **all modalities at inferenceÂ time** (though you can zeroâ€‘pad missing ones).\n",
    "\n",
    "> **ðŸ’¡Â TryÂ This:** Benchmark at least **two** fusion baselines against gated fusion:  \n",
    "> * **Uniformâ€‘average logits** â€“ $ \\tilde{f} = \\frac{1}{M}\\sum_m f_m $.  \n",
    "> * **Uniformâ€‘average encodings** â€“ average branch embeddings *before* the classification head.  \n",
    "> * **Concatenateâ€‘thenâ€‘MLP** â€“ stack embeddings and learn one big predictor (classic early fusion).  \n",
    "> * **Confidenceâ€‘weighted average** â€“ weight logits by their inverse entropy.  \n",
    "> * **Cross-Attention Fusion** â€“  \n",
    "> Report BALANCED ACCURACY and plot how fusion weights evolve over training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7072ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalClassifier(nn.Module):\n",
    "    \"\"\"Three branches â†’ gated lateâ€‘fusion.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # phot + spectra\n",
    "        self.phot = PhotometryEncoder()\n",
    "        self.spec = SpectraEncoder()\n",
    "        fusion_dim = cfg.d_model + cfg.fc_units_spec\n",
    "        self.head  = nn.Sequential(nn.Linear(fusion_dim, 512),\n",
    "                                   nn.ReLU(), nn.Dropout(0.3),\n",
    "                                   nn.Linear(512, NUM_CLASSES))\n",
    "\n",
    "        # astroMiNN\n",
    "        self.astro = astroMiNN()\n",
    "\n",
    "        # gated fusion\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(NUM_CLASSES * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, phot, mask, spec, meta, img):\n",
    "        h_phot = self.phot(phot, mask)\n",
    "        h_spec = self.spec(spec)\n",
    "        logits_ps = self.head(torch.cat([h_phot, h_spec], 1))\n",
    "        logits_im = self.astro(meta, img)\n",
    "        stacked   = torch.cat([logits_ps, logits_im], 1)\n",
    "        w = torch.softmax(self.gate(stacked), 1)\n",
    "        final_logits = w[:, 0:1] * logits_ps + w[:, 1:2] * logits_im\n",
    "        return final_logits\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Î³â€‘weighted focal loss with optional Î± balancing.\"\"\"\n",
    "    def __init__(self, gamma=cfg.gamma, alpha=None):\n",
    "        super().__init__()\n",
    "        self.g, self.a = gamma, alpha\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        logp = F.log_softmax(logits, 1); p = logp.exp()\n",
    "        idx  = torch.arange(logits.size(0), device=logits.device)\n",
    "        logp_t, p_t = logp[idx, target], p[idx, target]\n",
    "        loss = -((1 - p_t) ** self.g) * logp_t\n",
    "        if self.a is not None:\n",
    "            loss *= self.a[target]\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81d42c",
   "metadata": {},
   "source": [
    "## 6 Â· Data Preparation\n",
    "\n",
    "Pipeline summary:\n",
    "\n",
    "1. **File sanity check:** drop rows whose `.npy` bundle is missing.  \n",
    "2. **Random 80â€¯% /Â 20â€¯% split** for train/val (*stratified* would be nicerâ€”exercise!).  \n",
    "3. **Spectra StandardScaler** fitted on training set only.  \n",
    "4. **Photometry channel meanâ€¯Â±â€¯std** for zâ€‘score normalisation.  \n",
    "5. **Classâ€‘balanced Î±** for focalâ€‘loss: \\( \\alpha_c âˆ 1/\\sqrt{N_c} \\).\n",
    "\n",
    "> **ðŸ’¡Â TryÂ This:** instead of `StandardScaler`, fit a **RobustScaler** to downâ€‘weight spectral outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727936a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1Â Load CSV and sanityâ€‘check files\n",
    "exist_mask = full_df['file'].apply(lambda f: (Path(cfg.root_path) / f).exists())\n",
    "missing = (~exist_mask).sum()\n",
    "if missing:\n",
    "    warnings.warn(f\"dropping {missing} samples with missing .npy\")\n",
    "full_df = full_df[exist_mask].reset_index(drop=True)\n",
    "\n",
    "# 6.2Â Train/val split --------------------------------------------------------\n",
    "train_df, val_df = np.split(full_df.sample(frac=1, random_state=cfg.seed),\n",
    "                            [int(0.8 * len(full_df))])\n",
    "\n",
    "# 6.3Â Spectra scaler ---------------------------------------------------------\n",
    "spec_scaler = StandardScaler()\n",
    "for f in train_df['file']:\n",
    "    arr = np.nan_to_num(np.load(Path(cfg.root_path) / f, allow_pickle=True).item()['spectra'][0])\n",
    "    spec_scaler.partial_fit(arr.reshape(1, -1))\n",
    "\n",
    "# 6.4Â Datasets & loaders -----------------------------------------------------\n",
    "train_ds = MultiModalDataset(train_df, cfg.root_path, spec_scaler=spec_scaler,\n",
    "                             grid_size=cfg.grid_size)\n",
    "phot_mean, phot_std = compute_phot_mean_std(train_ds)\n",
    "train_ds.phot_mean, train_ds.phot_std = phot_mean, phot_std\n",
    "\n",
    "val_ds = MultiModalDataset(val_df, cfg.root_path, phot_mean, phot_std,\n",
    "                           spec_scaler, cfg.grid_size)\n",
    "\n",
    "collate = build_collate(phot_mean, phot_std)\n",
    "train_ld = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                      num_workers=cfg.num_workers, collate_fn=collate, drop_last=True)\n",
    "val_ld   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                      num_workers=cfg.num_workers, collate_fn=collate)\n",
    "\n",
    "# 6.5Â Î±Â balancing for focal loss --------------------------------------------\n",
    "labels_train = torch.tensor([train_ds[i][-1] for i in range(len(train_ds))])\n",
    "counts = torch.bincount(labels_train, minlength=NUM_CLASSES).float()\n",
    "alpha  = (1.0 / torch.sqrt(counts)).to(device)\n",
    "\n",
    "print('âœ”ï¸  Data ready. Train batches:', len(train_ld), '| Val batches:', len(val_ld))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354a265",
   "metadata": {},
   "source": [
    "## 7 Â· Training Loop\n",
    "\n",
    "We use:\n",
    "\n",
    "* **AdamW** optimiser  \n",
    "* **Cosineâ€‘Annealingâ€‘Warmâ€‘Restarts** LR schedule  \n",
    "* **Focal loss** with Î³â€¯=â€¯2 to fight class imbalance  \n",
    "* **Gradient clipping** at 1.0Â norm  \n",
    "* **Early stopping** on *macro AUPRC* with patienceâ€¯=â€¯50 epochs\n",
    "\n",
    "Keep an eye on **WeightsÂ &Â Biases** dashboards: learning curves, ROC, PR, and gate weights per batch.\n",
    "\n",
    "> **ðŸ’¡Â TryÂ This:** try early stopping on other criteria, such as Balanced Accuracy or Validation Loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=cfg.project, mode=cfg.wandb_mode, config=vars(cfg))\n",
    "\n",
    "model = MultiModalClassifier().to(device)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "crit  = FocalLoss(cfg.gamma, alpha)\n",
    "\n",
    "best_auprc, no_imp = 0.0, 0\n",
    "ckpt_path = Path(wandb.run.dir) / 'best.pt'\n",
    "\n",
    "# -------------------------------------------------------\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    tl, correct, N = 0., 0, 0\n",
    "    all_prob, all_lab = [], []\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for phot, mask, spec, meta, img, y in loader:\n",
    "            phot, mask, spec = phot.to(device), mask.to(device), spec.to(device)\n",
    "            meta, img, y     = meta.to(device), img.to(device), y.to(device)\n",
    "\n",
    "            logits = model(phot, mask, spec, meta, img)\n",
    "            loss   = crit(logits, y)\n",
    "\n",
    "            if train:\n",
    "                opt.zero_grad(); loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                opt.step(); sched.step()\n",
    "\n",
    "            tl += loss.item() * y.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "            N += y.size(0)\n",
    "            all_prob.append(F.softmax(logits, 1).detach().cpu())\n",
    "            all_lab.append(y.cpu())\n",
    "\n",
    "    probs = torch.cat(all_prob); labs = torch.cat(all_lab)\n",
    "    acc   = correct / N\n",
    "    auroc = roc_auc_score(labs, probs, multi_class='ovr', average='macro')\n",
    "    return tl / N, acc, auroc, labs.numpy(), probs.numpy()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "for ep in range(1, cfg.epochs + 1):\n",
    "    tr_loss, tr_acc, tr_auc, *_ = run_epoch(train_ld, True)\n",
    "    va_loss, va_acc, va_auc, yt, yp = run_epoch(val_ld, False)\n",
    "\n",
    "    y_onehot = label_binarize(yt, classes=np.arange(NUM_CLASSES))\n",
    "    va_auprc = average_precision_score(y_onehot, yp, average='macro')\n",
    "\n",
    "    wandb.log({'epoch': ep,\n",
    "               'train/loss': tr_loss, 'train/acc': tr_acc,\n",
    "               'val/loss': va_loss, 'val/acc': va_acc,\n",
    "               'val/auroc_macro': va_auc, 'val/auprc_macro': va_auprc,\n",
    "               'lr': sched.get_last_lr()[0]})\n",
    "\n",
    "    if va_auprc > best_auprc:\n",
    "        best_auprc, no_imp = va_auprc, 0\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        wandb.save(str(ckpt_path))\n",
    "    else:\n",
    "        no_imp += 1\n",
    "        if no_imp >= cfg.patience:\n",
    "            print('â¹ Early stopping'); break\n",
    "    print(f'[ep {ep:03d}] tr_acc={tr_acc:.3f}  va_acc={va_acc:.3f}  va_AUPRC={va_auprc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c8717",
   "metadata": {},
   "source": [
    "## 8 Â· Evaluation & Visualisation\n",
    "\n",
    "For each split we log:\n",
    "\n",
    "* *Metrics*: Acc, F1â€‘macro, AUROCâ€‘macro, AUPRCâ€‘macro, Topâ€‘2/3 accuracy  \n",
    "* *Plots*: ROC curve per class, PR curve, confusion matrix, confidence histograms  \n",
    "\n",
    "Interpretation tips:\n",
    "\n",
    "* **ROC vsÂ PR:** PR is more sensitive to class imbalanceâ€”optimise AUPRC here.  \n",
    "* **Confidence histogram:** a wellâ€‘calibrated classifier shows sharp peaks nearâ€¯0 andâ€¯1 for each class.  \n",
    "* **Gate weights:** if $ w_\\text{phot} $ is systematically low, maybe the photometry encoder is underâ€‘trained. That is very common.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a08698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, split):\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    loss, acc, auc_macro, y, p = run_epoch(loader, False)\n",
    "    y_pred = p.argmax(1)\n",
    "    # Your code here...\n",
    "\n",
    "val_metrics = evaluate(val_ld, 'val')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278241ba-031e-41d0-8134-2e50df89d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1Â Test set ---------------------------------------------------------------\n",
    "\n",
    "## We will do this later!\n",
    "\n",
    "\n",
    "#test_df = pd.read_csv(cfg.test_csv_path)\n",
    "#test_df['type'] = test_df['type'].map(ORIG2BROAD)\n",
    "#test_ds = MultiModalDataset(test_df, cfg.test_root, phot_mean, phot_std,\n",
    "                            spec_scaler, cfg.grid_size)\n",
    "#test_ld = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                     #num_workers=cfg.num_workers, collate_fn=collate)\n",
    "#test_metrics = evaluate(test_ld, 'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4b108",
   "metadata": {},
   "source": [
    "## 9 Â· Next Steps & StretchÂ Goals \n",
    "\n",
    "| Goal | Suggestion |\n",
    "|------|------------|\n",
    "| **Better Encoders** | Try different encoders for all modalities. Some times, less can be more! |\n",
    "| **Better Fusion** | No wrong answers here. Be creative! Talking about that... |\n",
    "| **Earlyâ€‘fusion Transformer** | Concatenate photometry fluxes, binned spectra, and PSF embeddings into one megaâ€‘sequence; add modality tokens. Take a look at *Perceiver I/O*! |\n",
    "| **Full taxonomy** | Expand `ORIG2BROAD` mapping to all subclasses; swap loss to classâ€‘balanced focal. |\n",
    "| **Earlyâ€‘fusion Transformer** | Concatenate photometry fluxes, binned spectra, and PSF embeddings into one megaâ€‘sequence; add modality tokens. |\n",
    "| **Coâ€‘attention (Interâ€‘fusion)** | Implement *LXMERT*â€‘style crossâ€‘attention between phot and spec encoders. |\n",
    "| **Selfâ€‘supervised preâ€‘training** | Maskedâ€‘value reconstruction on lightâ€‘curves; BYOL on PSF stamps. |\n",
    "| **Uncertaintyâ€‘aware gating** | Use logitsâ€™ variance or entropy to reâ€‘weight unreliable modalities. |\n",
    "| **Hyperâ€‘param sweep** | `wandb.sweep` on LR, Î³, #heads, experts. |\n",
    "| **Model ensembling** | Logitâ€‘average 5 checkpoints with different seeds. |\n",
    "\n",
    "\n",
    "**Leaderboard metric:** *BALANCED ACCURACY on the hidden testâ€‘set.*  \n",
    "Push your best run to Slack Day05, with some beautiful plots :)\n",
    "\n",
    "Happy hacking!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc91668-50bd-4a9a-b309-39dc6a8f17f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ztfss25",
   "language": "python",
   "name": "ztfss25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
