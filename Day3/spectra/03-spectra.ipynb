{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZTF Summer School 2025 - Day 3\n",
    "\n",
    "## Building a Deep Learning Model: A Case Study in Spectral Classification\n",
    "\n",
    "In this notebook, we will take a look at the process of how to build and train a model for spectral classification.  \n",
    "The same methodology can also be applied to other tasks involving convolutional networks (CNNs).\n",
    "\n",
    "---\n",
    "\n",
    "### We will follow these steps:\n",
    "\n",
    "1. Download and Prepare the Dataset  \n",
    "2. Understand the Dataset Structure and Meaning\n",
    "3. Know the Components of a Deep Learning Model\n",
    "4. Build a Model\n",
    "5. Train the Model on the Data  \n",
    "6. Analyze and Understand the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset Preparation\n",
    "\n",
    "The dataset has been preprocessed and split into three subsets:\n",
    "\n",
    "- `train.pt`: training set containing 560 samples per class\n",
    "- `val.pt`: validation set containing 120 samples per class\n",
    "- `test.pt`: test set containing 120 samples per class\n",
    "\n",
    "Each `.pt` file is a PyTorch-saved dictionary with the following structure:\n",
    "\n",
    "- `flux`: Tensor of shape `(N, 1, 4096)`, representing normalized and interpolated spectral flux data\n",
    "- `labels`: List of integer class labels corresponding to each spectrum\n",
    "\n",
    "These files are ready to be directly loaded into PyTorch `Dataset` and `DataLoader` classes for training, validation, and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "Link:https://drive.google.com/drive/folders/1nQP6WPMmfGSIQzhK7soLeSWMw2-7tHK9?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from exercise import *\n",
    "class_order = ['SN Ia', 'SN Ib', 'SN Ic', 'SN IIP', 'SN IIb', 'SN IIn', 'AGN', 'Cataclysmic', 'TDE']\n",
    "plot_label_distribution_from_pt(pt_dir=\"Data\", class_order=class_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flux_examples_three_panels(\n",
    "    pt_dir=\"Data\",\n",
    "    wavelength_range=(3850, 8500),\n",
    "    interp_len=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset and Dataloader Explanation\n",
    "\n",
    "The code below defines how we load the spectral data stored in `.pt` format using PyTorch's `Dataset` and `DataLoader` classes.\n",
    "\n",
    "####  `SpectraPTDataset`\n",
    "\n",
    "This custom dataset class does the following:\n",
    "\n",
    "- Loads a `.pt` file containing:\n",
    "  - `flux`: a tensor of shape `(N, 1, 4096)` for each interpolated spectrum\n",
    "  - `labels`: a list of class labels (either strings or integers)\n",
    "- Optionally maps string labels to class indices using `class_order`\n",
    "- Returns each sample as `(x, y)`:\n",
    "  - `x`: normalized flux tensor of shape `(1, 4096)`\n",
    "  - `y`: integer class label\n",
    "\n",
    "####  `create_data_loaders(config)`\n",
    "\n",
    "This utility function:\n",
    "\n",
    "- Creates `train`, `val`, and `test` datasets using `SpectraPTDataset`\n",
    "- Wraps them with PyTorch `DataLoader`s\n",
    "  - `shuffle=True` for training\n",
    "  - `shuffle=False` for validation and test\n",
    "- Returns the three loaders and the class index order\n",
    "\n",
    "> These loaders will be used during model training, validation, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class SpectraPTDataset(Dataset):\n",
    "    def __init__(self, pt_path, label_key='labels', class_order=None):\n",
    "        data = torch.load(pt_path)\n",
    "        self.flux = data['flux']\n",
    "        self.labels = data[label_key]\n",
    "\n",
    "        assert self.flux.shape[0] == len(self.labels), \"Mismatch in flux and redshift length\"\n",
    "\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(class_order)} if class_order else None\n",
    "\n",
    "        if self.class_to_idx and isinstance(self.labels[0], str):\n",
    "            self.labels = [self.class_to_idx[label] for label in self.labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.flux.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.flux[idx]\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)  # [1, 4096]\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "    \n",
    "    \n",
    "def create_data_loaders(config):\n",
    "    class_order = config['class_order'] \n",
    "\n",
    "    train_dataset = SpectraPTDataset(\n",
    "        pt_path=config['train_dir'],\n",
    "        label_key='labels',\n",
    "        class_order=class_order\n",
    "    )\n",
    "    val_dataset = SpectraPTDataset(\n",
    "        pt_path=config['val_dir'],\n",
    "        label_key='labels',\n",
    "        class_order=class_order\n",
    "    )\n",
    "    test_dataset = SpectraPTDataset(\n",
    "        pt_path=config['test_dir'],\n",
    "        label_key='labels',\n",
    "        class_order=class_order\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['num_workers'])\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'])\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'])\n",
    "\n",
    "    return train_loader, val_loader, test_loader, class_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training and Validation Loop Explanation\n",
    "\n",
    "We define two core functions to handle model training and evaluation:\n",
    "\n",
    "---\n",
    "\n",
    "####  `train_one_epoch(model, loader, criterion, optimizer, device, scaler, max_grad_norm)`\n",
    "\n",
    "This function performs a **single training epoch**:\n",
    "\n",
    "- **Set model to training mode**: `model.train()`\n",
    "- **Loop over mini-batches** from `loader`:\n",
    "  - Move `sequence` and `target` to GPU/CPU\n",
    "  - Use **mixed precision** with `torch.amp.autocast` for faster training\n",
    "  - Compute predictions and loss\n",
    "  - Backpropagate using `scaler.scale(loss).backward()`\n",
    "  - Clip gradients to prevent exploding gradients\n",
    "  - Update model parameters and scaler\n",
    "- Tracks running average of:\n",
    "  - `loss`\n",
    "  - `accuracy` (top-1)\n",
    "\n",
    " **Returns**: average loss and accuracy of the epoch\n",
    "\n",
    "---\n",
    "\n",
    "####  `validate(model, loader, criterion, device, class_names, epoch_type=\"val\")`\n",
    "\n",
    "This function evaluates the model on **validation or test set**:\n",
    "\n",
    "- Set model to evaluation mode: `model.eval()`\n",
    "- Disable gradients: `torch.no_grad()`\n",
    "- Loop over batches and:\n",
    "  - Compute predictions and loss\n",
    "  - Use `torch.softmax` to get class probabilities\n",
    "  - Compute:\n",
    "    - Top-1 Accuracy\n",
    "    - Top-3 Accuracy\n",
    "    - Macro F1 Score\n",
    "- Optionally plots confusion matrix for test set\n",
    "\n",
    " **Returns**: `loss`, `top-1 accuracy`, and `macro F1 score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, scaler, max_grad_norm=1.0):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "\n",
    "    for sequence, target in progress_bar:\n",
    "        sequence, target = sequence.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "            outputs = model(sequence)\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += preds.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item(), acc=100. * correct / total)\n",
    "\n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def validate(model, loader, criterion, device, class_names, epoch_type=\"val\"):\n",
    "    model.eval()\n",
    "    total, correct, correct_top3, running_loss = 0, 0, 0, 0.0\n",
    "    all_preds, all_targets = [], []\n",
    "    progress_bar = tqdm(loader, desc=\"Validation\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequence, target in progress_bar:\n",
    "            sequence, target = sequence.to(device), target.to(device)\n",
    "\n",
    "            with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "                output = model(sequence)\n",
    "                loss = criterion(output, target)\n",
    "                probs = torch.softmax(output, dim=1)\n",
    "\n",
    "            preds = output.argmax(dim=1)\n",
    "            correct += preds.eq(target).sum().item()\n",
    "            _, top3 = output.topk(3, dim=1)\n",
    "            correct_top3 += (top3 == target.view(-1, 1)).any(dim=1).sum().item()\n",
    "            total += target.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item(), acc=100. * correct / total)\n",
    "\n",
    "    val_loss = running_loss / len(loader)\n",
    "    val_acc = 100. * correct / total\n",
    "    val_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "    if epoch_type == \"test\":\n",
    "        plot_confusion_matrix_double(all_targets, all_preds, class_names, dataset_type=\"Test\")\n",
    "\n",
    "    return val_loss, val_acc, val_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common Loss Functions\n",
    "\n",
    "This note covers the most commonly used loss functions in classification and regression tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Classification Losses\n",
    "\n",
    "####  Binary Cross Entropy (BCE)\n",
    "\n",
    "- **Use case**: Binary classification (e.g., 0 vs 1)\n",
    "- **Prediction format**: Output should be a probability in [0, 1] (use Sigmoid)\n",
    "- **PyTorch**:\n",
    "  - `nn.BCELoss()` — requires manual `Sigmoid`\n",
    "  - `nn.BCEWithLogitsLoss()` — includes `Sigmoid` internally (recommended)\n",
    "\n",
    "---\n",
    "\n",
    "####  Cross Entropy Loss\n",
    "\n",
    "- **Use case**: Multi-class classification\n",
    "- **Prediction format**:\n",
    "  - Targets: class indices (e.g., 0, 1, 2, ...)\n",
    "  - Predictions: raw logits (no `Softmax` needed)\n",
    "  where \\( C \\) is the number of classes.\n",
    "- **PyTorch**: `nn.CrossEntropyLoss()` (automatically applies `LogSoftmax`)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Regression Losses\n",
    "\n",
    "####  Mean Squared Error (MSE)\n",
    "\n",
    "- **Use case**: General regression tasks\n",
    "- **Properties**: Sensitive to outliers; penalizes large errors more\n",
    "- **PyTorch**: `nn.MSELoss()`\n",
    "\n",
    "---\n",
    "\n",
    "####  Mean Absolute Error (MAE)\n",
    "\n",
    "- **Use case**: Regression with outliers\n",
    "\n",
    "- **Properties**: More robust to outliers; less sensitive to large errors\n",
    "- **PyTorch**: `nn.L1Loss()`\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary Table\n",
    "\n",
    "| Task          | Loss Function     | Description                            | PyTorch API            |\n",
    "|---------------|-------------------|----------------------------------------|-------------------------|\n",
    "| Classification | Binary Cross Entropy | For binary classification             | `nn.BCEWithLogitsLoss()` |\n",
    "| Classification | Cross Entropy       | For multi-class classification        | `nn.CrossEntropyLoss()`   |\n",
    "| Regression     | Mean Squared Error  | Penalizes large errors (sensitive)    | `nn.MSELoss()`            |\n",
    "| Regression     | Mean Absolute Error | Robust to outliers                    | `nn.L1Loss()`             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optimizer vs Scheduler\n",
    "\n",
    "<img src=\"assets/Optimizer.png\" alt=\"Optimizer\" width=\"400\">\n",
    "\n",
    "- **Optimizer**: Updates model weights using gradient from loss.\n",
    "- **Scheduler**: Dynamically adjusts learning rate used by the optimizer.\n",
    "\n",
    "---\n",
    "\n",
    "###  Training Step Flow\n",
    "\n",
    "```python\n",
    "# Forward pass\n",
    "outputs = model(inputs)\n",
    "loss = loss_fn(outputs, targets)   # ← 1. Compute loss\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()                    # ← 2. Compute gradients\n",
    "\n",
    "# Optimizer step\n",
    "optimizer.step()                   # ← 3. Update parameters using gradients\n",
    "\n",
    "# Learning rate scheduler step\n",
    "scheduler.step()                   # ← 4. Adjust learning rate for next step\n",
    "```\n",
    "\n",
    "### Analogy Table: Understanding the Training Components\n",
    "\n",
    "| Concept      | Real-World Role       | Responsibility                        | Analogy Explanation |\n",
    "|--------------|------------------------|----------------------------------------|----------------------|\n",
    "| **Loss**     | Map / Destination      | Measures how far off the model is     | Like a GPS showing how far you are from your target |\n",
    "| **Gradient** | Navigation Directions  | Tells the direction to adjust weights | Like GPS arrows telling you which way to turn |\n",
    "| **Optimizer**| Driver / Steering Wheel| Updates model parameters               | The driver turning the wheel to follow the path |\n",
    "| **Scheduler**| Gas Pedal Controller   | Adjusts learning rate (speed)          | Controls how fast the car moves—slow at start (warmup), then speeds up, then slows down again (decay) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Common Optimizers: SGD, SGD + Momentum, Adam, AdamW\n",
    "\n",
    "| Optimizer         | Momentum | Adaptive LR | Description                                        | Pros                                         | Cons                                      | Typical Use Cases                        |\n",
    "|-------------------|----------|-------------|----------------------------------------------------|----------------------------------------------|-------------------------------------------|-------------------------------------------|\n",
    "| **SGD**           | ❌        | ❌           | Basic stochastic gradient descent                  | Very simple, transparent behavior             | Converges slowly, easily trapped in minima | Very small models, baseline comparison     |\n",
    "| **SGD + Momentum**|  ✅       | ❌           | Adds velocity to SGD to smooth and accelerate updates | Strong generalization, CV standard choice     | Sensitive to learning rate and momentum     | Large-scale CNNs (ResNet, EfficientNet), image classification |\n",
    "| **Adam**          |  ✅       |    ✅        | Combines momentum and per-parameter adaptive LR    | Fast convergence, easy to use                 | May overfit, slightly worse generalization  | NLP, small data, fast prototyping          |\n",
    "| **AdamW**         |    ✅     |    ✅        | Adam with decoupled weight decay for better regularization | Fast + stable, strong for modern deep nets    | Slightly higher memory usage                | Transformers, ViT, large CNNs              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Learning Schedule \n",
    "\n",
    "<img src=\"assets/LR_Schedule.png\" alt=\"LR-Schedule\" width=\"800\">\n",
    "\n",
    "| Scheduler                    | Mechanism                                 | Key Parameters                                           | LR Trend                       | Cyclic | Recommended Scenarios                             |\n",
    "|-----------------------------|-------------------------------------------|----------------------------------------------------------|--------------------------------|--------|---------------------------------------------------|\n",
    "| **Constant LR**             | Keep learning rate constant               | `lr`                                                     | Constant                       | ❌     | Simple models, debugging                         |\n",
    "| **StepLR**                  | Decay LR every fixed number of epochs     | `step_size`, `gamma`                                     | Step-wise decay               | ❌     | Classic CNNs like ResNet, VGG                   |\n",
    "| **MultiStepLR**             | Decay LR at specified epochs              | `milestones`, `gamma`                                    | Multi-step decay              | ❌     | Fine-tuning with empirical control              |\n",
    "| **ExponentialLR**           | Multiply LR by γ every epoch              | `gamma`                                                  | Exponential decay             | ❌     | Small models or fast convergence                |\n",
    "| **CosineAnnealingLR**       | Cosine decay to minimum                   | `T_max`, `eta_min`                                       | One-shot cosine decay         | ❌     | Transformers, vision models                     |\n",
    "| **CosineAnnealingWarmRestarts** | Multiple cosine decays with restarts         | `T_0`, `T_mult`, `eta_min`                               | Cosine waves with restarts    |  ✅    | Pretraining large models, escaping local minima |\n",
    "| **CyclicLR**                | Oscillate between `base_lr` and `max_lr`  | `base_lr`, `max_lr`, `step_size_up`, `mode`              | Cyclic up and down            |   ✅   | NLP, large-scale training, saddle point escape  |\n",
    "| **OneCycleLR**              | Increase then sharply decrease LR         | `max_lr`, `steps_per_epoch`, `epochs`, `pct_start`       | One-shot triangular schedule  | ❌     | Modern DNNs, transfer learning, fine-tuning     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Main Training Function Explained (`main()`)\n",
    "\n",
    "The `main()` function manages the full pipeline from training to evaluation. It consists of several key stages:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Initialization\n",
    "\n",
    "- `torch.cuda.synchronize()` ensures CUDA is ready before training.\n",
    "- `set_seed()` ensures reproducibility across runs.\n",
    "- `get_device()` detects whether to use GPU or CPU.\n",
    "- `print_config()` logs current hyperparameter settings.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Data Loading\n",
    "\n",
    "- `create_data_loaders(config)` loads `.pt` files for training, validation, and testing.\n",
    "- Returns `train_loader`, `val_loader`, `test_loader`, and `class_names` (label order).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Model, Optimizer, Scheduler Setup\n",
    "\n",
    "- `build_model(config)` builds a model.\n",
    "- `build_optimizer()` sets up AdamW optimizer with given learning rate and weight decay.\n",
    "- `build_scheduler()` sets up a learning rate scheduler (warmup + cosine annealing).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Class-Balanced Loss & Mixed Precision Setup\n",
    "\n",
    "- `FocalLoss` is used to handle class imbalance, with weights computed from `train_labels`.\n",
    "- If labels are strings, they're mapped to integer indices using `class_names`.\n",
    "- `torch.amp.GradScaler()` enables automatic mixed precision for faster training.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Training Loop\n",
    "\n",
    "For each epoch:\n",
    "- Call `train_one_epoch()`:\n",
    "  - Performs forward + backward pass, gradient clipping, optimizer step\n",
    "  - Tracks average loss and top-1 accuracy\n",
    "- Call `validate()` on validation set:\n",
    "  - Computes validation loss, accuracy, macro F1\n",
    "- If validation F1 improves:\n",
    "  - Save the best model checkpoint (`best_model.pth`)\n",
    "  - Reset `no_improve_epochs` counter\n",
    "- If not:\n",
    "  - Increment `no_improve_epochs`\n",
    "- If early stopping triggered:\n",
    "  - Stop training early to prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Final Testing\n",
    "\n",
    "- If mode is `'train'`, the best checkpoint is reloaded.\n",
    "- `validate()` is called on the test set.\n",
    "- Final test loss, accuracy, and F1 score are printed and returned.\n",
    "\n",
    "---\n",
    "\n",
    "####  Final Output\n",
    "\n",
    "Returns:\n",
    "- Best test F1 score (for logging or hyperparameter tuning)\n",
    "- A dictionary of test metrics: `f1`, `acc`, `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(trial=None, config=None, mode='train', build_model_fn=None):\n",
    "    import torch.nn as nn\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    set_seed(config.get('seed', 42))\n",
    "    print_config(config, trial)\n",
    "    device = get_device()\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader, val_loader, test_loader, class_names = create_data_loaders(config)\n",
    "\n",
    "    # Build model\n",
    "    model =  build_model(config).to(device)\n",
    "\n",
    "    # optimizer \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "\n",
    "    #scheduler\n",
    "    warmup_epochs = config.get('warmup_epochs', 5)\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=config['start_factor'],\n",
    "        end_factor=config['end_factor'],\n",
    "        total_iters=warmup_epochs\n",
    "    )\n",
    "    cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=config['T_0'],\n",
    "        T_mult=config['T_mult'],\n",
    "        eta_min=config['eta_min']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "        milestones=[warmup_epochs]\n",
    "    )\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    no_improve_epochs = 0\n",
    "    save_dir = config.get(\"model_save_dir\", \"../checkpoints\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"\\n[Epoch {epoch+1}/{config['epochs']}]\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, scaler,\n",
    "            max_grad_norm=1.0  \n",
    "        )\n",
    "\n",
    "        val_loss, val_acc, val_f1 = validate(\n",
    "            model, val_loader, criterion, device, class_names, epoch_type=\"val\"\n",
    "        )\n",
    "\n",
    "        print(f\"📊 [Val @ Epoch {epoch+1}] Acc: {val_acc:.2f}%, F1: {val_f1:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_f1 > best_val_f1 + 1e-6:\n",
    "            best_val_f1 = val_f1\n",
    "            no_improve_epochs = 0\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "            print(f\">>> 🚀🚀🚀 New Best F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            print(f\">>> No F1 improvement for {no_improve_epochs} epoch(s)\")\n",
    "\n",
    "        if early_stopping(no_improve_epochs, config['patience']):\n",
    "            print(\">>> Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        clean_memory()\n",
    "\n",
    "    print(f\"\\n Best F1 on Validation: {best_val_f1:.4f}\")\n",
    "\n",
    "    # Directly evaluate on test set\n",
    "    print(\"\\n>>> Final Testing on Test Set...\")\n",
    "    model.load_state_dict(torch.load(os.path.join(save_dir, \"best_model.pth\")))\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc, test_f1 = validate(\n",
    "        model, test_loader, criterion, device, class_names, epoch_type=\"test\"\n",
    "    )\n",
    "\n",
    "    print(f\">>> Test Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%, F1: {test_f1:.4f}\")\n",
    "\n",
    "    return test_f1, {\n",
    "        \"f1\": test_f1,\n",
    "        \"acc\": test_acc,\n",
    "        \"loss\": test_loss\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pt_dataset_dir = \"Data\"\n",
    "model_save_dir = os.path.join(pt_dataset_dir, 'model')\n",
    "final_config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 0.8e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"focal_loss_gamma\": 2.0,\n",
    "\n",
    "    \"warmup_epochs\": 10,\n",
    "    \"T_0\": 5,\n",
    "    \"T_mult\": 1,\n",
    "    \"eta_min\": 1e-5,\n",
    "    \"start_factor\": 1e-6,\n",
    "    \"end_factor\": 1.0,\n",
    "    \"num_workers\": 0,\n",
    "    \"sampling\": False,\n",
    "    \"epochs\": 100,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"patience\": 5,\n",
    "\n",
    "    \"train_dir\": os.path.join(pt_dataset_dir,\"train.pt\"),  \n",
    "    \"val_dir\": os.path.join(pt_dataset_dir,\"val.pt\"),\n",
    "    \"test_dir\": os.path.join(pt_dataset_dir, \"test.pt\"),\n",
    "    \"model_save_dir\": os.path.join(pt_dataset_dir, \"model\"),\n",
    "    \"class_order\": ['SN Ia', 'SN Ib', 'SN Ic', 'SN IIP', 'SN IIb', 'SN IIn', 'AGN', 'Cataclysmic',  'TDE'],\n",
    "    \"seed\": 42,\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.1 Convolution\n",
    "\n",
    "Convolution: Convolution is a fundamental operation in deep learning that applies learnable filters to extract local patterns from input sequences or signals.\n",
    "\n",
    "\n",
    "<img src=\"assets/1D-CNN.gif\" alt=\"Convolution-1D\" width=\"600\">\n",
    "\n",
    "\n",
    "For a 1D convolution length calculation:\n",
    "\n",
    "$$\n",
    "L_\\text{out} = \\left\\lfloor \\frac{L_\\text{in} + 2P - K}{S} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Where:**\n",
    "\n",
    "| Symbol        | Meaning                      |\n",
    "|---------------|-------------------------------|\n",
    "| L_in          | Input length                  |\n",
    "| K             | Kernel size                   |\n",
    "| P             | Padding size                  |\n",
    "| S             | Stride                        |\n",
    "| L_out         | Output length                 |\n",
    "| floor(x)      | Floor (round down to integer) |\n",
    "\n",
    "```python\n",
    "conv = nn.Conv1d(\n",
    "    in_channels=1,\n",
    "    out_channels=4,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=1\n",
    ")\n",
    "```\n",
    "\n",
    "**Given:**\n",
    "\n",
    "- Input length $L_{\\text{in}} = 10$\n",
    "- Kernel size $K = 3$\n",
    "- Padding $P = 1$\n",
    "- Stride $S = 1$\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "$$\n",
    "L_{\\text{out}} = \\left\\lfloor \\frac{10 + 2 \\times 1 - 3}{1} \\right\\rfloor + 1 = \\left\\lfloor \\frac{9}{1} \\right\\rfloor + 1 = 10\n",
    "$$\n",
    "\n",
    " **Output length = 10** (same as input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise  1\n",
    "If input shape = [8, 16, 2048],  [Batch_size,Channel_number,data_length]\n",
    "\n",
    "Format: [Batch_size, Channel_number, Data_length]\n",
    "\n",
    "```python\n",
    "conv = nn.Conv1d(in_channels=8, out_channels=64, kernel_size=16, stride=4, padding=4),  \n",
    "```\n",
    "\n",
    "what is the output shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "Answer = []\n",
    "check_question_one(Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2\n",
    "\n",
    "If input shape = [4, 32, 1024]  \n",
    "Format: [Batch_size, Channel_number, Data_length]\n",
    "\n",
    "There are two convolutional layers:\n",
    "\n",
    "```python\n",
    "conv1 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=9, stride=3, padding=2)\n",
    "conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "Answer = []\n",
    "check_question_second(Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Pooling\n",
    "\n",
    "Pooling: a non-learnable operation in deep learning that reduces the spatial or temporal resolution by summarizing local regions, typically using max, average, or min values.\n",
    "\n",
    "\n",
    "<img src=\"assets/MaxPool.png\" alt=\"MaxPool\" width=\"600\">\n",
    "\n",
    "\n",
    "For a 1D pool length calculation:\n",
    "\n",
    "$$\n",
    "L_\\text{out} = \\left\\lfloor \\frac{L_\\text{in} - K}{S} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Where:**\n",
    "\n",
    "| Symbol        | Meaning                      |\n",
    "|---------------|-------------------------------|\n",
    "| L_in          | Input length                  |\n",
    "| K             | Kernel size                   |\n",
    "| S             | Stride                        |\n",
    "| L_out         | Output length                 |\n",
    "| floor(x)      | Floor (round down to integer) |\n",
    "\n",
    "```python\n",
    "conv = nn.MaxPool1d(\n",
    "    kernel_size=4,\n",
    "    stride=4,\n",
    ")\n",
    "```\n",
    "\n",
    "**Given:**\n",
    "\n",
    "- Input length $L_{\\text{in}} = 16$\n",
    "- Kernel size $K = 4$\n",
    "- Stride $S = 4$\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "$$\n",
    "L_{\\text{out}} = \\left\\lfloor \\frac{16 - 4}{4} \\right\\rfloor + 1 = \\left\\lfloor \\frac{12}{4} \\right\\rfloor + 1 = 4\n",
    "$$\n",
    "\n",
    " **Output length = 4** (1/4 length compare to  input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3\n",
    "\n",
    "If input shape = [2, 16, 50]  \n",
    "Format: [Batch_size, Channel_number, Data_length]\n",
    "\n",
    "Apply the following pooling layer:\n",
    "\n",
    "```python\n",
    "pool = nn.MaxPool1d(kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "Answer =  []\n",
    "\n",
    "check_question_third(Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 4: MaxPool1d on Specific Data\n",
    "\n",
    "Given the following 1D input tensor (length = 10):\n",
    "\n",
    "```python\n",
    "x = torch.tensor([[1., 3., 2., 5., 0., 4., 7., 8., 6., 1.]])\n",
    "```\n",
    "\n",
    "Use:\n",
    "```python\n",
    "pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:\n",
    "Answer =[]\n",
    "\n",
    "\n",
    "check_question_fourth(Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 6: AvgPool1d on Specific Data\n",
    "\n",
    "Given the following 1D input tensor (length = 10):\n",
    "\n",
    "```python\n",
    "x = torch.tensor([[1., 3., 2., 5., 0., 4., 7., 8., 6., 1.]])\n",
    "```\n",
    "\n",
    "Use:\n",
    "```python\n",
    "pool = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "Answer = []\n",
    "\n",
    "check_question_sixth(Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.3 Normalization \n",
    "<img src=\"assets/Normalization.webp\" alt=\"Normalization\" width=\"800\">\n",
    "\n",
    "Comparison of LayerNorm, BatchNorm, GroupNorm, and InstanceNorm\n",
    "\n",
    "**LayerNorm**, **BatchNorm**, **GroupNorm**, and **InstanceNorm** are four commonly used normalization techniques. They all aim to stabilize training and accelerate convergence, but they differ in terms of **normalization dimensions, typical use cases, and dependence on batch size**.\n",
    "\n",
    "---\n",
    "\n",
    "Summary of Differences\n",
    "\n",
    "| Feature                  | **BatchNorm**                                      | **LayerNorm**                                    | **GroupNorm**                                    | **InstanceNorm**                                 |\n",
    "|--------------------------|----------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|\n",
    "| **Normalization Axis**   | Batch + Channel (per channel)                     | All features per sample (e.g., C×H×W)            | Per group of channels within each sample         | Per channel per sample (e.g., H×W per channel)    |\n",
    "| **Batch Size Dependent** |  Yes (batch size must be > 1)                   | ❌ No                                             | ❌ No                                             | ❌ No                                             |\n",
    "| **Typical Use Case**     | CNNs for images, need large batch size            | Transformers, RNNs, sequence models              | CNNs with small batch sizes or needing stability | Style transfer, image generation, instance-specific normalization |\n",
    "| **Trainable Parameters** | γ, β (learnable scaling and shifting)             | γ, β                                              | γ, β                                              | γ, β                                              |\n",
    "| **In Images**            | `(N, C, H, W)` → normalize over `N×H×W` per channel | `(N, C, H, W)` → normalize over `C×H×W` per sample | `(N, C, H, W)` → normalize `H×W` within each group of channels | `(N, C, H, W)` → normalize `H×W` per channel per sample |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Activation Function\n",
    "\n",
    "<img src=\"assets/Activation.png\" alt=\"Normalization\" width=\"800\">\n",
    "\n",
    "###### Comparison Table\n",
    "\n",
    "| Activation |  Advantages                                                  | Disadvantages                                             | Typical Use Case                        | Smooth | Differentiable | Vanishing Gradient | Computation | **Equation** | **Derivative** |\n",
    "|------------|---------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------------|-----------|-------------------|------------------------|----------------|--------------|----------------|\n",
    "| **ReLU**   | Simple and fast, good for positive gradients                  | Zero gradient for x < 0 (dying neurons)                     | CNNs, image classification                 | ❌ No     |  Yes            | 🔺 High (negative)     | ⭐ Very Low     | $\\max(0, x)$ | $1$ if $x>0$, $0$ otherwise |\n",
    "| **Leaky ReLU** | Fixes dead neurons, has small gradient for x < 0        | Slight noise from negative slope                            | Improved ReLU, CNNs                        | ❌ No     |  Yes            | ⚠️ Medium              | ⭐ Very Low     | $\\max(\\alpha x, x)$ | $1$ if $x>0$, $\\alpha$ otherwise |\n",
    "| **ELU**    | Smooth, allows negative values, better convergence            | Needs $\\alpha$, uses exponent                              | Deep nets, avoiding dying neurons          |  Yes    |  Yes            | 🔻 Low                 | ⭐⭐ Medium      | $\\begin{cases}x & x\\ge0 \\\\ \\alpha(e^x-1) & x<0\\end{cases}$ | $\\begin{cases}1 & x\\ge0 \\\\ \\alpha e^x & x<0\\end{cases}$ |\n",
    "| **Sigmoid**| Output in [0, 1], good for probabilities                      | Saturates easily, slow, non-zero-centered                   | Binary output, old RNNs                    |  Yes    |  Yes            | 🔺 Very High           | ⭐⭐ Medium      | $\\frac{1}{1+e^{-x}}$ | $\\sigma(x)(1-\\sigma(x))$ |\n",
    "| **Tanh**   | Zero-centered, smooth output                                  | Still saturates at large inputs                            | RNNs, sequential modeling                  |  Yes    |  Yes            | 🔺 High                | ⭐⭐ Medium      | $\\tanh(x)$ | $1 - \\tanh^2(x)$ |\n",
    "| **GELU**   | Smooth nonlinear gating, performs well in transformers       | Complex, slower than ReLU                                  | Transformers (BERT, GPT)                   |  Yes    |  Yes            | 🔻 Low                 | ⭐⭐⭐ Moderate   | $x \\cdot \\Phi(x)$ | $\\Phi(x) + x \\cdot \\phi(x)$ |\n",
    "| **Swish**  | Learnable gating, smooth, better than ReLU in many cases     | Uses sigmoid internally, slower                            | EfficientNet, deep architectures           |  Yes    |  Yes            | 🔻 Low                 | ⭐⭐⭐ Moderate   | $x \\cdot \\sigma(x)$ | $\\sigma(x) + x \\cdot \\sigma(x)(1 - \\sigma(x))$ |\n",
    "| **Maxout** | Can learn any convex function, avoids vanishing gradients     | Parameter-heavy, expensive                                 | Special architectures, learned activations |  Yes    |  Yes            | 🔽 Very Low            | ⭐⭐⭐⭐ High      | $\\max(w_1^Tx + b_1, w_2^Tx + b_2)$ | Piecewise linear (depends on max branch) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Regularization\n",
    "\n",
    "<img src=\"assets/dropout.png\" alt=\"Dropout\" width=\"800\">\n",
    "\n",
    "#### Dropout\n",
    "- **Dropout**: Prevent overfitting by randomly dropping individual neurons during training.  \n",
    "- Example usage: `nn.Dropout(p=0.5)`  \n",
    "- Commonly applied after **fully connected (FC)** layers or **feature maps**.\n",
    "- During training, neurons are randomly \"turned off\" with a given probability `p`, forcing the model to learn more robust representations.\n",
    "\n",
    "---\n",
    "\n",
    "#### DropPath (Stochastic Depth)\n",
    "\n",
    "- **DropPath** (also called **Stochastic Depth**) is a regularization technique used in **deep residual networks**.\n",
    "- Instead of dropping individual neurons like Dropout, **entire residual branches (paths) are randomly skipped** during training.\n",
    "- It helps to:\n",
    "  - Regularize deep networks (e.g., ResNet-50, Vision Transformers)\n",
    "  - Prevent overfitting in very deep architectures\n",
    "  - Encourage different paths to learn independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.6 Linear Layer (Fully Connected)\n",
    "\n",
    "The `Linear` layer is a core component in neural networks, often used after feature extraction to **fuse information globally** or map it to the **output space**.\n",
    "\n",
    "---\n",
    "\n",
    "A `Linear` (fully connected) layer performs a **learnable affine transformation**:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$ is the input vector (shape: `[in_features]`)\n",
    "- $\\mathbf{W}$ is the weight matrix (shape: `[out\\_features, in\\_features]`)\n",
    "- $\\mathbf{b}$ is the bias vector (shape: `[out\\_features]`)\n",
    "- $\\mathbf{y}$ is the output vector (shape: `[out_features]`)\n",
    "\n",
    "---\n",
    "\n",
    "###  Usage in PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Example: Map a 256-dim vector to 10 output classes\n",
    "linear = nn.Linear(in_features=256, out_features=10)\n",
    "# Input: [batch_size, 256]\n",
    "x = torch.randn(32, 256)\n",
    "# Output: [batch_size, 10]\n",
    "output = linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.2 Structure of a Network\n",
    "\n",
    "In general, when we build a Convolutional Neural Network (CNN), the architecture typically consists of the following stages:\n",
    "\n",
    "---\n",
    "\n",
    "###  1. Feature Extraction Layers\n",
    "\n",
    "- **Convolution**: Extract local features  \n",
    "- **Pooling**: Downsample and reduce spatial size  \n",
    "- **Normalization**: Stabilize training (e.g., `BatchNorm`, `LayerNorm`)  \n",
    "- **Activation**: Introduce non-linearity (e.g., `ReLU`, `GELU`)  \n",
    "- **Dropout**: Regularization to prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "###  2. Fully Connected Layers\n",
    "\n",
    "- **Flatten**: Convert multi-dimensional feature maps to a 1D vector  \n",
    "- **Linear (Dense)**: Perform global reasoning  \n",
    "- **Activation**: Further non-linearity (e.g., `ReLU`, `Swish`)  \n",
    "- **Normalization**: Stabilize training (e.g., `BatchNorm`, `LayerNorm`)  \n",
    "- **Dropout**: Additional regularization (optional)\n",
    "\n",
    "---\n",
    "\n",
    "###  3. Output Layer\n",
    "\n",
    "- **For classification**: Produces logits or probability scores  \n",
    "- **For regression**: Outputs continuous values\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "### **CNN model**\n",
    "- 2 convolutional layers:\n",
    "  - `Conv1D(1 → 32, kernel_size=3, padding=1)` followed by ReLU.\n",
    "  - `Conv1D(32 → 64, kernel_size=3, padding=1)` followed by ReLU.\n",
    "- `MaxPool1D(kernel_size=2)` to reduce sequence length from `4096 → 2048`.\n",
    "- A `Flatten` layer to convert `[B, 64, 2048]` → `[B, 131072]`.\n",
    "\n",
    "### **Classifier (MLP)**\n",
    "- `Linear(131072 → 256)` followed by ReLU.\n",
    "- `Dropout(p=0.3)` for regularization.\n",
    "- `Linear(256 → 9)` for class logits.\n",
    "\n",
    "### **Input / Output**\n",
    "- **Input:** 1D spectral vector of shape `[B, 1, 4096]`.\n",
    "- **Output:** Class logits of shape `[B, 9]` for 9 target classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_model(config):\n",
    "    class BaseLineSpectraCNN(nn.Module):\n",
    "        def __init__(self, input_channels=1, input_length=4096, num_classes=9):\n",
    "            super().__init__()\n",
    "\n",
    "            self.conv_block = nn.Sequential(\n",
    "                nn.Conv1d(input_channels, 32, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2)  # → [B, 64, 2048]\n",
    "            )\n",
    "\n",
    "            reduced_length = input_length // 2  # after pooling\n",
    "            self.flatten_dim = 64 * reduced_length\n",
    "\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(self.flatten_dim, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(256, len(config['class_order']))\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv_block(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.mlp(x)\n",
    "\n",
    "    return BaseLineSpectraCNN(\n",
    "        input_channels=1,\n",
    "        input_length=4096,\n",
    "        num_classes=len(config['class_order'])\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "test_score, test_metrics = main(config=final_config, build_model_fn=build_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise: GalSpecNet Architecture\n",
    "\n",
    "---\n",
    "\n",
    "###  Input\n",
    "\n",
    "- **Spectral Input:** `[B, 1, 4096]`\n",
    "\n",
    "---\n",
    "\n",
    "###  Convolution + Pooling Layers\n",
    "\n",
    "1. **Method:** `Conv1D + ReLU`  \n",
    "   → **Output:** `[B, 64, 4096]`\n",
    "\n",
    "2. **Method:** `MaxPool1D`  \n",
    "   → **Output:** `[B, 64, 1024]`\n",
    "\n",
    "3. **Method:** `Conv1D + ReLU`  \n",
    "   → **Output:** `[B, 128, 1024]`\n",
    "\n",
    "4. **Method:** `MaxPool1D`  \n",
    "   → **Output:** `[B, 128, 256]`\n",
    "\n",
    "5. **Method:** `Conv1D + ReLU`  \n",
    "   → **Output:** `[B, 256, 256]`\n",
    "\n",
    "6. **Method:** `MaxPool1D`  \n",
    "   → **Output:** `[B, 256, 64]`\n",
    "\n",
    "7. **Method:** `Conv1D + ReLU`  \n",
    "   → **Output:** `[B, 32, 64]`\n",
    "\n",
    "---\n",
    "\n",
    "###  Flatten\n",
    "\n",
    "- `[B, 32, 64]` → `[B, 2048]`\n",
    "\n",
    "---\n",
    "\n",
    "###  Classifier (MLP)\n",
    "\n",
    "8. **Method:** `Dropout(0.3)`  \n",
    "   → **Output:** `[B, 2048]`\n",
    "\n",
    "9. **Method:** `Linear + ReLU`  \n",
    "   → **Output:** `[B, 64]`\n",
    "\n",
    "10. **Method:** `Linear + ReLU`  \n",
    "    → **Output:** `[B, 32]`\n",
    "\n",
    "11. **Method:** `Linear`  \n",
    "    → **Output:** `[B, 9]`\n",
    "\n",
    "---\n",
    "\n",
    "###  Output\n",
    "\n",
    "- **Method:** `Softmax`  \n",
    "  → **Output:** Class probabilities `[B, 9]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instructions:\n",
    "# Replace each ___ below with appropriate values according to the structure.\n",
    "# Do NOT change the layer order or add/remove layers.\n",
    "\n",
    "def build_model(config):\n",
    "    class GalSpecNet(nn.Module):\n",
    "        def __init__(self, input_channels=1, input_length=4096, num_classes=9):\n",
    "            super().__init__()\n",
    "\n",
    "            #  Feature Extractor\n",
    "            self.feature_extractor = nn.Sequential(\n",
    "                # TODO 1: Fill in first Conv1D layer to output [B, 64, 4096]\n",
    "                nn.Conv1d(input_channels, ___, kernel_size=___, stride= 1, padding=___),  \n",
    "                nn.ReLU(),\n",
    "                # TODO 2: Add MaxPool1d to downsample to [B, 64, 1024]\n",
    "                nn.MaxPool1d(kernel_size=___,stride=___),\n",
    "\n",
    "                # TODO 3: Second Conv1D to go from 64 → 128 channels\n",
    "                nn.Conv1d(___, ___, kernel_size=___,stride=1, padding=___),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=___, stride=__),  # [B, 128, 256]\n",
    "\n",
    "                # TODO 4: Third Conv1D to go from 128 → 256 channels\n",
    "                nn.Conv1d(___, ___, kernel_size=___,stride=1, padding=___),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=___,stride=__),  # [B, 256, 64]\n",
    "\n",
    "                # TODO 5: Final Conv1D to go from 256 → 32 channels, output [B, 32, 64]\n",
    "                nn.Conv1d(___, ___, kernel_size=___,stride=1, padding=___),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "            #  Flatten\n",
    "            # TODO 6: What should the flatten size be?\n",
    "            self.flatten_dim = ___ * ___  # = 2048\n",
    "\n",
    "            #  MLP Classifier\n",
    "            self.classifier = nn.Sequential(\n",
    "                # TODO 7: Add Dropout layer with correct probability\n",
    "                nn.Dropout(p=___),  # [B, 2048]\n",
    "                # TODO 8: Fully connected layers\n",
    "                nn.Linear(___, ___),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(___, ___),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(___, len(config['class_order']))  # [B, 9]\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.feature_extractor(x)\n",
    "            x = x.view(x.size(0), -1)  # flatten to [B, 2048]\n",
    "            return self.classifier(x)\n",
    "\n",
    "    return GalSpecNet(\n",
    "        input_channels=1,\n",
    "        input_length=4096,\n",
    "        num_classes=len(config['class_order'])\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "        \"class_order\": list(range(9))  # mock 9 classes\n",
    "    }\n",
    "model = build_model(config)\n",
    "x = torch.randn(2, 1, 4096)  # random input: batch_size=2\n",
    "y = model(x)\n",
    "\n",
    "print(\" Output shape:\", y.shape)  # should be [2, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Inference and Evaluation on Spectral Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Path to the ONNX model and test data\n",
    "onnx_path = \"Data/model/3850-8500.onnx\"\n",
    "pt_path = \"Data/test.pt\"\n",
    "\n",
    "# Load the test data\n",
    "data = torch.load(pt_path)\n",
    "x = data['flux']\n",
    "y_true = data['labels']\n",
    "x_np = x.numpy().astype(np.float32)\n",
    "\n",
    "if x_np.ndim == 2:\n",
    "    x_np = np.expand_dims(x_np, axis=1)  # Ensure shape is [N, 1, 4096]\n",
    "\n",
    "# Load the ONNX model\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run([output_name], {input_name: x_np})  # [N, num_classes]\n",
    "predictions = np.argmax(outputs[0], axis=1)               # [N]\n",
    "\n",
    "# Convert string labels to index and compare predictions\n",
    "class_order = ['SN Ia', 'SN Ib', 'SN Ic', 'SN IIP', 'SN IIb', 'SN IIn', 'AGN', 'Cataclysmic', 'TDE', 'SLSN-I']\n",
    "y_true_np = np.array([class_order.index(label) for label in y_true])\n",
    "y_pred_names = [class_order[i] for i in predictions[:30]]\n",
    "y_true_names = [class_order[i] for i in y_true_np[:30]]\n",
    "\n",
    "# Print comparison results\n",
    "print(\"\\n🧾 Prediction vs Ground Truth:\")\n",
    "for i, (pred, true) in enumerate(zip(y_pred_names, y_true_names)):\n",
    "    print(f\"Sample {i:02d} | Predicted: {pred:<12} | True: {true}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AppleCider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
